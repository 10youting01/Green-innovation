{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francia/anaconda3/envs/csr_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained('bert-large-cased')\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../CSR_report_processed_v4_gemini_v0/NASDAQ_AAL_2007_v0_gemini_corrected.txt'\n",
    "# try:\n",
    "#     with open(path, 'r') as file:\n",
    "#         text = file.read()\n",
    "# except:\n",
    "#     print(f\"Error: The file '{path}' was not found.\")\n",
    "#     exit(1)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=512\n",
    "chunk_size = max_len - 2\n",
    "input_folder = \"../CSR_report_processed_v4_gemini_v0/CSR_report_new_collect\"\n",
    "output_csv = \"../output_dataset/CSR_report_new_collect/csr_embeddings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    # 兩個換行符號代表下一個段落\n",
    "    paragraphs = [p.strip() for p in text.split(r'\\n\\s*\\n') if p.strip()]\n",
    "    embeddings = []\n",
    "    for paragraph in paragraphs:\n",
    "        tokens = tokenizer.tokenize(paragraph)\n",
    "        chuncks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "        for chunck in chuncks:\n",
    "            tokens_chunck = ['[CLS]'] + chunck + ['[SEP]']\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens_chunck)\n",
    "            input_ids = torch.tensor([input_ids]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids)\n",
    "                last_hidden_states = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "            token_embeddings = last_hidden_states[1:-1]  # Exclude [CLS] and [SEP]\n",
    "            chunck_embedding = token_embeddings.mean(dim=0)\n",
    "            embeddings.append(chunck_embedding)\n",
    "    doc_embedding = torch.stack(embeddings).mean(dim=0)\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_csv):\n",
    "    df = pd.DataFrame(columns=[\"file_name\"] + [f\"dim_{i}\" for i in range(1024)])\n",
    "    df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_files = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Processing: NASDAQ_AMZN_2019\n",
      "🚀 Processing: NASDAQ_BRKS_2019\n",
      "🚀 Processing: NASDAQ_GILD_2016\n",
      "🚀 Processing: NASDAQ_GILD_2017\n",
      "🚀 Processing: NASDAQ_GILD_2018\n",
      "🚀 Processing: NASDAQ_GILD_2019\n",
      "🚀 Processing: NASDAQ_MIDD_2019\n",
      "🚀 Processing: NYSE_AES_2015\n",
      "🚀 Processing: NYSE_AES_2016\n",
      "🚀 Processing: NYSE_AES_2017\n",
      "🚀 Processing: NYSE_AKO-B_2014\n",
      "🚀 Processing: NYSE_AKO-B_2015\n",
      "🚀 Processing: NYSE_AKO-B_2016\n",
      "🚀 Processing: NYSE_AKO-B_2017\n",
      "🚀 Processing: NYSE_AKO-B_2018\n",
      "🚀 Processing: NYSE_AKO-B_2019\n",
      "🚀 Processing: NYSE_AVY_2019\n",
      "🚀 Processing: NYSE_CDP_2014\n",
      "🚀 Processing: NYSE_CDP_2016\n",
      "🚀 Processing: NYSE_CDP_2017\n",
      "🚀 Processing: NYSE_CDP_2018\n",
      "🚀 Processing: NYSE_CDP_2019\n",
      "🚀 Processing: NYSE_CHU_2016\n",
      "🚀 Processing: NYSE_CHU_2017\n",
      "🚀 Processing: NYSE_CHU_2018\n",
      "🚀 Processing: NYSE_CHU_2019\n",
      "🚀 Processing: NYSE_COR_2016\n",
      "🚀 Processing: NYSE_COR_2017\n",
      "🚀 Processing: NYSE_COR_2018\n",
      "🚀 Processing: NYSE_COR_2019\n",
      "🚀 Processing: NYSE_CRM_2012\n",
      "🚀 Processing: NYSE_CRM_2014\n",
      "🚀 Processing: NYSE_CRM_2016\n",
      "🚀 Processing: NYSE_CRM_2017\n",
      "🚀 Processing: NYSE_CRM_2018\n",
      "🚀 Processing: NYSE_CRM_2019\n",
      "🚀 Processing: NYSE_CTB_2014\n",
      "🚀 Processing: NYSE_CTB_2016\n",
      "🚀 Processing: NYSE_CTB_2018\n",
      "🚀 Processing: NYSE_CTB_2019\n",
      "🚀 Processing: NYSE_CX_2003\n",
      "🚀 Processing: NYSE_CX_2004\n",
      "🚀 Processing: NYSE_CX_2005\n",
      "🚀 Processing: NYSE_CX_2006\n",
      "🚀 Processing: NYSE_CX_2007\n",
      "🚀 Processing: NYSE_CX_2008\n",
      "🚀 Processing: NYSE_CX_2009\n",
      "🚀 Processing: NYSE_CX_2010\n",
      "🚀 Processing: NYSE_CX_2011\n",
      "🚀 Processing: NYSE_CX_2012\n",
      "🚀 Processing: NYSE_CX_2013\n",
      "🚀 Processing: NYSE_CX_2014\n",
      "🚀 Processing: NYSE_CX_2015\n",
      "🚀 Processing: NYSE_CX_2016\n",
      "🚀 Processing: NYSE_CX_2017\n",
      "🚀 Processing: NYSE_CX_2018\n",
      "🚀 Processing: NYSE_CX_2019\n",
      "🚀 Processing: NYSE_DB_2002\n",
      "🚀 Processing: NYSE_DB_2003\n",
      "🚀 Processing: NYSE_DB_2004\n",
      "🚀 Processing: NYSE_DB_2005\n",
      "🚀 Processing: NYSE_DB_2006\n",
      "🚀 Processing: NYSE_DB_2007\n",
      "🚀 Processing: NYSE_DB_2008\n",
      "🚀 Processing: NYSE_DB_2009\n",
      "🚀 Processing: NYSE_DB_2010\n",
      "🚀 Processing: NYSE_DB_2011\n",
      "🚀 Processing: NYSE_DB_2012\n",
      "🚀 Processing: NYSE_DB_2013\n",
      "🚀 Processing: NYSE_DB_2014\n",
      "🚀 Processing: NYSE_DB_2015\n",
      "🚀 Processing: NYSE_DB_2016\n",
      "🚀 Processing: NYSE_DB_2017\n",
      "🚀 Processing: NYSE_DB_2018\n",
      "🚀 Processing: NYSE_DB_2019\n",
      "🚀 Processing: NYSE_ELP_2018\n",
      "🚀 Processing: NYSE_GAU_2014\n",
      "🚀 Processing: NYSE_GAU_2015\n",
      "🚀 Processing: NYSE_GAU_2016\n",
      "🚀 Processing: NYSE_GAU_2017\n",
      "🚀 Processing: NYSE_GAU_2018\n",
      "🚀 Processing: NYSE_GAU_2019\n",
      "🚀 Processing: NYSE_GE_2019\n",
      "🚀 Processing: NYSE_G_2017\n",
      "🚀 Processing: NYSE_G_2019\n",
      "🚀 Processing: NYSE_HAL_2015\n",
      "🚀 Processing: NYSE_HAL_2016\n",
      "🚀 Processing: NYSE_HAL_2017\n",
      "🚀 Processing: NYSE_HAL_2018\n",
      "🚀 Processing: NYSE_HAL_2019\n",
      "🚀 Processing: NYSE_HSBC_2012\n",
      "🚀 Processing: NYSE_HSBC_2013\n",
      "🚀 Processing: NYSE_HSBC_2014\n",
      "🚀 Processing: NYSE_HSBC_2015\n",
      "🚀 Processing: NYSE_HSBC_2016\n",
      "🚀 Processing: NYSE_HSBC_2017\n",
      "🚀 Processing: NYSE_HSBC_2018\n",
      "🚀 Processing: NYSE_HSBC_2019\n",
      "🚀 Processing: NYSE_IHG_2015\n",
      "🚀 Processing: NYSE_IHG_2016\n",
      "🚀 Processing: NYSE_IHG_2017\n",
      "🚀 Processing: NYSE_IHG_2018\n",
      "🚀 Processing: NYSE_IHG_2019\n",
      "🚀 Processing: NYSE_KDSKF_2017\n",
      "🚀 Processing: NYSE_KDSKF_2018\n",
      "🚀 Processing: NYSE_KDSKF_2019\n",
      "🚀 Processing: NYSE_KOF_2017\n",
      "🚀 Processing: NYSE_KOF_2018\n",
      "🚀 Processing: NYSE_KOF_2019\n",
      "🚀 Processing: NYSE_LFC_2014\n",
      "🚀 Processing: NYSE_LFC_2015\n",
      "🚀 Processing: NYSE_LFC_2017\n",
      "🚀 Processing: NYSE_LFC_2018\n",
      "🚀 Processing: NYSE_LFC_2019\n",
      "🚀 Processing: NYSE_LTM_2013\n",
      "🚀 Processing: NYSE_LTM_2014\n",
      "🚀 Processing: NYSE_LTM_2015\n",
      "🚀 Processing: NYSE_LTM_2016\n",
      "🚀 Processing: NYSE_LTM_2017\n",
      "🚀 Processing: NYSE_LTM_2018\n",
      "🚀 Processing: NYSE_LTM_2019\n",
      "🚀 Processing: NYSE_MDT_2014\n",
      "🚀 Processing: NYSE_MDT_2016\n",
      "🚀 Processing: NYSE_MDT_2017\n",
      "🚀 Processing: NYSE_MDT_2018\n",
      "🚀 Processing: NYSE_MDT_2019\n",
      "🚀 Processing: NYSE_MFG_2005\n",
      "🚀 Processing: NYSE_MFG_2006\n",
      "🚀 Processing: NYSE_MFG_2007\n",
      "🚀 Processing: NYSE_MFG_2008\n",
      "🚀 Processing: NYSE_MFG_2009\n",
      "🚀 Processing: NYSE_MFG_2010\n",
      "🚀 Processing: NYSE_MFG_2011\n",
      "🚀 Processing: NYSE_MFG_2012\n",
      "🚀 Processing: NYSE_MFG_2013\n",
      "🚀 Processing: NYSE_MFG_2014\n",
      "🚀 Processing: NYSE_MFG_2015\n",
      "🚀 Processing: NYSE_MFG_2016\n",
      "🚀 Processing: NYSE_MFG_2017\n",
      "🚀 Processing: NYSE_MFG_2018\n",
      "🚀 Processing: NYSE_MFG_2019\n",
      "🚀 Processing: NYSE_POR_2017\n",
      "🚀 Processing: NYSE_POR_2018\n",
      "🚀 Processing: NYSE_POR_2019\n",
      "🚀 Processing: NYSE_RACE_2017\n",
      "🚀 Processing: NYSE_RACE_2018\n",
      "🚀 Processing: NYSE_RACE_2019\n",
      "🚀 Processing: NYSE_RBC_2018\n",
      "🚀 Processing: NYSE_RBC_2019\n",
      "🚀 Processing: NYSE_SPB_2017\n",
      "🚀 Processing: NYSE_TTC_2019\n"
     ]
    }
   ],
   "source": [
    "processed_count = 0\n",
    "\n",
    "for fname in sorted(os.listdir(input_folder)):\n",
    "    if fname.endswith(\".txt\") and processed_count < max_files:\n",
    "        try:\n",
    "            full_path = os.path.join(input_folder, fname)\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            file_name = fname.split('_v0')[0]\n",
    "\n",
    "            # 避免重複處理\n",
    "            existing_df = pd.read_csv(output_csv)\n",
    "            if file_name in existing_df[\"file_name\"].values:\n",
    "                print(f\"✔️ Already processed: {file_name}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"🚀 Processing: {file_name}\")\n",
    "            emb = get_embedding(text)\n",
    "\n",
    "            # 儲存結果\n",
    "            row = [file_name] + emb.tolist()\n",
    "            df_new = pd.DataFrame([row], columns=[\"file_name\"] + [f\"dim_{i}\" for i in range(1024)])\n",
    "            df_new.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {fname}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csr_env",
   "language": "python",
   "name": "csr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
