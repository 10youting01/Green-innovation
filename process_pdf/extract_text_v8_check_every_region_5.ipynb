{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660ecaa6-c434-4f16-8ba4-f4e73b41b4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdf2image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytesseract\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pytesseract\u001b[38;5;241m.\u001b[39mpytesseract\u001b[38;5;241m.\u001b[39mtesseract_cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr/bin/tesseract\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpdf2image\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_from_path\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdf2image'"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = \"/usr/bin/tesseract\"\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from statistics import quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9808e1-43f5-4e2f-a6e6-86ff7e46e2a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_path = \"../CSR Reporting/NASDAQ/NASDAQ_TEAM_2021.pdf\"  # æ›¿æ›ç‚ºæ‚¨çš„ PDF è·¯å¾‘\n",
    "output_image_path = \"../CSR_report_processed_v3/NASDAQ/NASDAQ_TEAM_2021\"\n",
    "output_file_path = \"../CSR_report_processed_v3/NASDAQ/NASDAQ_TEAM_2021/NASDAQ_TEAM_2021.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e0398-18fd-415a-9e86-f1f1b072d103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path, output_image_path, density_threshold=15):\n",
    "\n",
    "    if not os.path.exists(output_image_path):\n",
    "        os.makedirs(output_image_path)\n",
    "    # è½‰æ› PDF é é¢ç‚ºåœ–ç‰‡\n",
    "    pages = convert_from_path(pdf_path, dpi=300)\n",
    "    all_dense_regions = []\n",
    "    all_region_data = []\n",
    "\n",
    "    for page_i, page_image in enumerate(pages):\n",
    "        print(f\"\\n--- Processing Page {page_i + 1} ---\")\n",
    "\n",
    "        # OCR æ–‡å­—å€å¡Šæå–\n",
    "        data = pytesseract.image_to_data(page_image, output_type=pytesseract.Output.DICT)\n",
    "\n",
    "        if len(data['text']) < 300:\n",
    "            continue\n",
    "\n",
    "        valid_indices = [i for i in range(len(data['text'])) if data['text'][i].strip()]\n",
    "\n",
    "        if not valid_indices:\n",
    "            print(f\"Page {page_i + 1}: No valid text found.\")\n",
    "            continue\n",
    "\n",
    "        # è¨ˆç®—å¹³å‡å¯¬åº¦èˆ‡é«˜åº¦ä½œç‚ºé–¾å€¼\n",
    "        def trimmed_mean(values, trim_ratio=0.25):\n",
    "            sorted_values = np.sort(values)\n",
    "            trim_count = int(len(sorted_values) * trim_ratio)\n",
    "            trimmed_values = sorted_values[:trim_count]\n",
    "            return np.mean(trimmed_values) if len(trimmed_values) > 0 else 0\n",
    "\n",
    "        non_zero_widths = [data['width'][i] for i in valid_indices if data['width'][i] > 0]\n",
    "        non_zero_heights = [data['height'][i] for i in valid_indices if data['height'][i] > 0]\n",
    "\n",
    "        average_width = trimmed_mean(non_zero_widths)\n",
    "        average_height = sum(data['height'][i] for i in valid_indices) / len(valid_indices)\n",
    "        # average_height = trimmed_mean(non_zero_heights, trim_ratio=0.3)\n",
    "\n",
    "        # è¨­å®šåˆä½µé–¾å€¼\n",
    "        horizontal_threshold = int(average_width)\n",
    "        vertical_threshold = int(average_height)\n",
    "\n",
    "        print(f\"Avg Width: {average_width}, Avg Height: {average_height}\")\n",
    "        print(f\"Horizontal Threshold: {horizontal_threshold}, Vertical Threshold: {vertical_threshold}\")\n",
    "\n",
    "        # **å»ºç«‹æ‰€æœ‰æ–‡å­—å€å¡Š**\n",
    "        region_candidates = []\n",
    "        for i in valid_indices:\n",
    "            x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]\n",
    "            text = data['text'][i].strip()\n",
    "            text_length = len(text)\n",
    "            region_candidates.append([x, y, x + w, y + h, text_length, text, 1])  # 1 ä»£è¡¨é€™å€‹å€å¡Šå…§çš„æ–‡å­—æ•¸é‡\n",
    "\n",
    "        # è¨˜éŒ„æ‰€æœ‰çš„æ–‡å­—å€å¡Šè³‡è¨Š\n",
    "        region_data = []\n",
    "        for region in region_candidates:\n",
    "            region_data.append({\n",
    "                \"page\": page_i + 1,\n",
    "                \"x1\": region[0], \"y1\": region[1],\n",
    "                \"x2\": region[2], \"y2\": region[3],\n",
    "                \"width\": region[2] - region[0],\n",
    "                \"height\": region[3] - region[1],\n",
    "                \"text_length\": region[4],\n",
    "                \"text\": region[5],\n",
    "                \"text_num\": region[6]\n",
    "            })\n",
    "\n",
    "        all_region_data.extend(region_data)\n",
    "\n",
    "        # **å»ºç«‹å€å¡Šé—œä¿‚åœ–**\n",
    "        adjacency_list = {i: [] for i in range(len(region_candidates))}\n",
    "\n",
    "        # æª¢æŸ¥æ‰€æœ‰æ–‡å­—å€å¡Šæ˜¯å¦ç›¸é€£\n",
    "        for i, region1 in enumerate(region_candidates):\n",
    "            print(f\"Region {i}: {region}\")\n",
    "            x1, y1, x2, y2, _, _, _ = region1\n",
    "\n",
    "            for j, region2 in enumerate(region_candidates):\n",
    "                if i != j:\n",
    "                    x1b, y1b, x2b, y2b, _, _, _ = region2\n",
    "\n",
    "                    # åˆ¤æ–·å…©å€‹å€å¡Šæ˜¯å¦å¯ä»¥åˆä½µ\n",
    "                    horizontally_close = abs(x1 - x2b) <= horizontal_threshold or abs(x1b - x2) <= horizontal_threshold\n",
    "                    vertically_close = abs(y1 - y2b) <= vertical_threshold or abs(y1b - y2) <= vertical_threshold\n",
    "                    overlap_vertically = not (y1 > y2b or y2 < y1b)\n",
    "                    overlap_horizontally = not (x1 > x2b or x2 < x1b)\n",
    "\n",
    "                    if (horizontally_close or overlap_horizontally) and (vertically_close or overlap_vertically):\n",
    "                        adjacency_list[i].append(j)\n",
    "                        print(f\"Page {page_i + 1}: Connecting {region1[5]} -> {region2[5]}\")  # ğŸ”¥ Debug é€™è¡Œ\n",
    "\n",
    "        # **åˆä½µå€å¡Š**\n",
    "        visited = set()\n",
    "        merged_regions = []\n",
    "\n",
    "        def merge_connected_regions(start_idx):\n",
    "            print(f\"Starting BFS from: {region_candidates[start_idx][5]} at {region_candidates[start_idx][0]}, {region_candidates[start_idx][1]}\")\n",
    "\n",
    "            \"\"\" ä½¿ç”¨ BFS ä¾†åˆä½µæ‰€æœ‰ç›¸é€£å€å¡Š \"\"\"\n",
    "            queue = [start_idx]\n",
    "            merged_region = list(region_candidates[start_idx])  # è¤‡è£½åˆå§‹å€å¡Šè³‡è¨Š\n",
    "            visited.add(start_idx)\n",
    "            print(f\"Merging {region_candidates[start_idx][5]} at {region_candidates[start_idx][0]}, {region_candidates[start_idx][1]}\")\n",
    "\n",
    "            while queue:\n",
    "                idx = queue.pop(0)\n",
    "                for neighbor_idx in adjacency_list[idx]:\n",
    "                    if neighbor_idx not in visited:\n",
    "                        visited.add(neighbor_idx)\n",
    "                        queue.append(neighbor_idx)\n",
    "                        \n",
    "                        # åˆä½µå€å¡Š\n",
    "                        x1b, y1b, x2b, y2b, text_length, text, text_num = region_candidates[neighbor_idx]\n",
    "                        merged_region[0] = min(merged_region[0], x1b)\n",
    "                        merged_region[1] = min(merged_region[1], y1b)\n",
    "                        merged_region[2] = max(merged_region[2], x2b)\n",
    "                        merged_region[3] = max(merged_region[3], y2b)\n",
    "                        merged_region[4] += text_length\n",
    "                        merged_region[5] += \" \" + text\n",
    "                        merged_region[6] += text_num\n",
    "\n",
    "            return merged_region\n",
    "\n",
    "        # åŸ·è¡Œåˆä½µ\n",
    "        for i in range(len(region_candidates)):\n",
    "            if i not in visited:\n",
    "                merged_region = merge_connected_regions(i)\n",
    "                merged_regions.append(merged_region)\n",
    "\n",
    "        # **ç¯©é¸é«˜å¯†åº¦å€åŸŸ**\n",
    "        dense_regions = []\n",
    "        for region in merged_regions:\n",
    "            x1, y1, x2, y2, total_text_length, all_text, total_text_num = region\n",
    "            if total_text_num > density_threshold:\n",
    "                width = x2 - x1\n",
    "                height = y2 - y1\n",
    "                area = width * height\n",
    "                text_density = total_text_length / area if area > 0 else 0\n",
    "\n",
    "                if text_density >= 0.0001:\n",
    "                    dense_regions.append({\n",
    "                        \"page\": page_i + 1,\n",
    "                        \"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2,\n",
    "                        \"width\": width, \"height\": height, \"area\": area,\n",
    "                        \"total_text_length\": total_text_length,\n",
    "                        \"text_density\": text_density,\n",
    "                        \"all_text\": all_text.strip(),\n",
    "                        \"text_num\": total_text_num\n",
    "                    })\n",
    "        \n",
    "        # \n",
    "\n",
    "        all_dense_regions.extend(dense_regions)\n",
    "\n",
    "        # **ç¹ªè£½ç´…è‰²é‚Šæ¡†æ¨™è¨»å€åŸŸ**\n",
    "        draw = ImageDraw.Draw(page_image)\n",
    "        for region in dense_regions:\n",
    "            x1, y1, x2, y2 = region[\"x1\"], region[\"y1\"], region[\"x2\"], region[\"y2\"]\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "\n",
    "        page_image.save(f'{output_image_path}/page_{page_i + 1}.jpg')\n",
    "\n",
    "    # **è¼¸å‡º JSON**\n",
    "    with open(f'{output_image_path}/dense_regions.json', 'w') as json_file:\n",
    "        json.dump(all_dense_regions, json_file, indent=4)\n",
    "    \n",
    "    with open(f'{output_image_path}/region_candidates.json', 'w') as json_file:\n",
    "        json.dump(all_region_data, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632261f-4edf-4e3b-916a-6671be84d62b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m process_pdf(pdf_path, output_image_path, density_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36mprocess_pdf\u001b[0;34m(pdf_path, output_image_path, density_threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_pdf\u001b[39m(pdf_path, output_image_path, density_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_image_path):\n\u001b[1;32m      4\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(output_image_path)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# è½‰æ› PDF é é¢ç‚ºåœ–ç‰‡\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "process_pdf(pdf_path, output_image_path, density_threshold=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read second_round.json file\n",
    "# second_round  = json.load(open('C:/Users/r12725056/anaconda3/Master/Greenwashing/CSR_report_processed/NASDAQ/second_round/second_round.json', 'r'))\n",
    "# for i in range(0, 5):\n",
    "#     pdf_name = second_round[i]\n",
    "#     pdf_path = f'../CSR Reporting/NASDAQ/{pdf_name}.pdf'\n",
    "#     output_image_path = f'../CSR_report_processed_v3/NASDAQ/{pdf_name}/{pdf_name}_v3'\n",
    "#     if not os.path.exists(output_image_path):\n",
    "#         os.makedirs(output_image_path)\n",
    "#     process_pdf(pdf_path, output_image_path, density_threshold=15)\n",
    "#     print(f\"Processed {pdf_name} successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csr_env",
   "language": "python",
   "name": "csr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
