{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"../data/csr_embeddings_leq2019_citation_count_cpc_lagged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b31282",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed) # 控制 Python 內建的 random 模組（例如 random.shuffle()、random.randint()）。\n",
    "    np.random.seed(seed) # 控制 NumPy 所有隨機操作（例如 np.random.rand()、np.random.shuffle()）。\n",
    "    torch.manual_seed(seed) # 控制 CPU 上 PyTorch 的隨機性（例如 torch.rand()）。\n",
    "    torch.cuda.manual_seed(seed)  # for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True  # 強制使用 deterministic（確定性）版本的 CuDNN 操作，避免某些 kernel 造成非一致性輸出。\n",
    "    torch.backends.cudnn.benchmark = False     # 關閉 CuDNN 根據輸入自動選最佳演算法的機制，避免因選到不同 kernel 而有不同結果。\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd57bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8769ea3",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCSRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.x = torch.tensor(df[[f'dim_{i}' for i in range(1024)]].values, dtype=torch.float32)\n",
    "        y = df[['patents_count', 'total_5yr_forward_citations']].values\n",
    "        self.y = torch.log1p(torch.tensor(y, dtype=torch.float32))\n",
    "        self.indexes = df.index.tolist()\n",
    "        self.years = df[\"year\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx], self.indexes[idx], self.years[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9891cc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MTL(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256, head_dim=32):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, head_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.head_count = nn.Sequential(nn.Linear(head_dim, 1), nn.ReLU())\n",
    "        self.head_citation = nn.Sequential(nn.Linear(head_dim, 1), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):  # x: [B, 1024]\n",
    "        shared_out = self.shared(x)\n",
    "        y1 = self.head_count(shared_out)\n",
    "        y2 = self.head_citation(shared_out)\n",
    "        return torch.cat([y1, y2], dim=1)  # [B, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73539cf4",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metrics (expm1 to reverse log1p) ---\n",
    "def compute_metrics(y_pred, y_true):\n",
    "    y_pred = torch.expm1(y_pred).cpu().numpy()\n",
    "    y_true = torch.expm1(y_true).cpu().numpy()\n",
    "    metrics = {}\n",
    "\n",
    "    for i, name in enumerate([\"count\", \"citation\"]):\n",
    "        y_p, y_t = y_pred[:, i], y_true[:, i]\n",
    "        mse = np.mean((y_p - y_t) ** 2)\n",
    "        mae = np.mean(np.abs(y_p - y_t))\n",
    "        rmse = np.sqrt(mse)\n",
    "        smape = np.mean(2 * np.abs(y_p - y_t) / (np.abs(y_p) + np.abs(y_t) + 1e-8))*100\n",
    "        metrics[name] = {\"MSE\": mse, \"MAE\": mae, \"RMSE\": rmse, \"SMAPE\": smape}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8652b0",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEPunishLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, punish_mode='', return_each=False):\n",
    "        \"\"\"\n",
    "        :param alpha: 懲罰項權重\n",
    "        :param punish_mode: 'abs', 'square', 'binary', 'huber', 'ratio', 'threshold'\n",
    "        :param return_each: 是否回傳 count loss 和 citation loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.alpha = alpha\n",
    "        self.punish_mode = punish_mode\n",
    "        self.return_each = return_each\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        pred_count, pred_citation = preds[:, 0], preds[:, 1]\n",
    "        true_count, true_citation = targets[:, 0], targets[:, 1]\n",
    "\n",
    "        loss_count = self.mse(pred_count, true_count)\n",
    "        loss_citation = self.mse(pred_citation, true_citation)\n",
    "        loss_total = loss_count + loss_citation\n",
    "\n",
    "        # Initialize punishment\n",
    "        punishment = 0.0\n",
    "\n",
    "        if self.punish_mode:\n",
    "            mask = (true_count == 0)\n",
    "            if mask.sum() > 0:\n",
    "                if self.punish_mode == 'abs':\n",
    "                    punishment = torch.sum(torch.abs(pred_citation[mask]))\n",
    "                elif self.punish_mode == 'square':\n",
    "                    punishment = torch.sum((pred_citation[mask]) ** 2)\n",
    "                elif self.punish_mode == 'binary':\n",
    "                    punishment = torch.sum((pred_citation[mask] > 1e-4).float())\n",
    "                elif self.punish_mode == 'huber':\n",
    "                    punishment = nn.functional.smooth_l1_loss(\n",
    "                        pred_citation[mask],\n",
    "                        torch.zeros_like(pred_citation[mask]),\n",
    "                        reduction='sum'\n",
    "                    )\n",
    "                elif self.punish_mode == 'ratio':\n",
    "                    safe_count = true_count + 1e-6\n",
    "                    ratio = pred_citation / safe_count\n",
    "                    punishment = torch.sum(ratio[mask])\n",
    "                elif self.punish_mode == 'threshold':\n",
    "                    threshold = 1.0\n",
    "                    over = torch.clamp(pred_citation[mask] - threshold, min=0)\n",
    "                    punishment = torch.sum(over)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported punish_mode: {self.punish_mode}\")\n",
    "\n",
    "        loss_total = loss_total + self.alpha * punishment\n",
    "\n",
    "        if self.return_each:\n",
    "            return loss_total, loss_count.item(), loss_citation.item()\n",
    "        else:\n",
    "            return loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25a66a",
   "metadata": {},
   "source": [
    "## Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29333970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['year', 'ticker']).reset_index(drop=True)\n",
    "\n",
    "min_year = df['year'].min()\n",
    "max_year = df['year'].max()\n",
    "\n",
    "folds = []\n",
    "min_train_years = 5\n",
    "\n",
    "# expanding window loop\n",
    "for train_length in range(min_train_years, max_year - min_year + 1):\n",
    "    train_years = list(range(min_year, min_year + train_length))\n",
    "    val_year = min_year + train_length  # 只取下一年做驗證\n",
    "\n",
    "    train_df = df[df['year'].isin(train_years)].copy()\n",
    "    val_df = df[df['year'] == val_year].copy()\n",
    "\n",
    "    if not train_df.empty and not val_df.empty:\n",
    "        folds.append((train_df, val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728212b",
   "metadata": {},
   "source": [
    "## Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbad7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(model, val_loader, fold_id, best_epoch):\n",
    "    \"\"\"\n",
    "    回傳最佳模型在驗證集上的預測、標籤與 meta info（用於 CSV 儲存）\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    val_meta_info = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in enumerate(val_loader):\n",
    "            x_seqs, y_seqs, indices, years = zip(*batch)\n",
    "\n",
    "            x_seqs = torch.stack(x_seqs).to(next(model.parameters()).device)\n",
    "            y_seqs = torch.stack(y_seqs).to(next(model.parameters()).device)\n",
    "\n",
    "            preds = model(x_seqs)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_y.append(y_seqs.cpu())\n",
    "\n",
    "            for i, (index, year) in enumerate(zip(indices, years)):\n",
    "                val_meta_info.append({\n",
    "                    \"fold\": fold_id + 1,\n",
    "                    \"epoch\": best_epoch + 1,\n",
    "                    \"batch\": batch_id + 1,\n",
    "                    \"sample\": i,\n",
    "                    \"index\": index,\n",
    "                    \"year\": year\n",
    "                })\n",
    "\n",
    "    return [torch.cat(all_preds)], [torch.cat(all_y)], [val_meta_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0131c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Config ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 初始化儲存所有 fold 的 loss\n",
    "all_train_losses_total, all_train_losses_count, all_train_losses_citation = [], [], []\n",
    "all_train_preds_total, all_train_targets_total, all_train_meta_info_total = [], [], []\n",
    "\n",
    "all_val_losses_count, all_val_losses_citation ,all_val_losses_total = [], [], []\n",
    "all_val_preds_total, all_val_targets_total, all_val_meta_info_total = [], [], []\n",
    "\n",
    "best_epoch_list = []\n",
    "\n",
    "for fold_id, (train_df, val_df) in enumerate(folds):\n",
    "    print(f\"\\n====== Fold {fold_id+1} ({train_df['year'].min()}–{train_df['year'].max()} → {val_df['year'].iloc[0]}) ======\")\n",
    "\n",
    "    fold_train_preds_epochs, fold_train_targets_epochs, fold_train_meta_epochs = [], [], []\n",
    "    fold_val_preds_epochs, fold_val_targets_epochs, fold_val_meta_epochs = [], [], []\n",
    "    \n",
    "    train_dataset = SimpleCSRDataset(train_df)\n",
    "    val_dataset = SimpleCSRDataset(val_df)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: x, generator=generator)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "    # Model\n",
    "    model = MLP_MTL().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = MSEPunishLoss(alpha=1.0, punish_mode='', return_each=True)\n",
    "\n",
    "    # 儲存每個 epoch 的 loss（每 fold 一份）\n",
    "    train_loss_total_list, train_loss_count_list, train_loss_citation_list = [], [], []\n",
    "    val_loss_total_list, val_loss_count_list, val_loss_citation_list = [], [], []\n",
    "\n",
    "    # <-- Early Stopping 初始化 -->\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50\n",
    "    best_model_state = None\n",
    "    best_epoch = None  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_losses, epoch_loss_count_all, epoch_loss_citation_all = [], [], []\n",
    "        train_preds, train_targets, train_meta_info = [], [], []\n",
    "        for batch_id, batch in enumerate(train_loader):\n",
    "            x, y, idxs, years = zip(*batch)\n",
    "            x = torch.stack(x).to(device)\n",
    "            y = torch.stack(y).to(device)\n",
    "\n",
    "            preds = model(x)\n",
    "\n",
    "            loss_total, loss_count, loss_citation = criterion(preds, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss_total.item())\n",
    "            epoch_loss_count_all.append(loss_count)\n",
    "            epoch_loss_citation_all.append(loss_citation)\n",
    "\n",
    "            train_preds.append(preds.detach().cpu())\n",
    "            train_targets.append(y.detach().cpu())\n",
    "\n",
    "            for i, index in enumerate(idxs):\n",
    "                train_meta_info.append({\n",
    "                    \"fold\": fold_id + 1,\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"batch\": batch_id + 1,\n",
    "                    \"sample\": i,\n",
    "                    \"index\": index\n",
    "                })\n",
    "\n",
    "            # # ✔ 每個 batch 印 loss、predictions、truth\n",
    "            # print(f\"[Fold {fold_id+1}][Epoch {epoch+1}][Train][Batch] Company: {tickers[0]} | Samples: {len(indices)}\")\n",
    "            # print(f\"  Total Loss: {loss_total.item():.4f} | Count Loss: {loss_count:.4f} | Citation Loss: {loss_citation:.4f}\")\n",
    "            # print(f\"  Predictions (expm1, rounded): {np.round(torch.expm1(preds).detach().cpu().numpy())}\")\n",
    "            # print(f\"  Ground truth (expm1, rounded): {np.round(torch.expm1(y).detach().cpu().numpy())}\")\n",
    "            # print(f\"  Indices: {indices}\")\n",
    "\n",
    "        avg_train_loss = np.mean(epoch_losses)\n",
    "        avg_train_count = np.mean(epoch_loss_count_all)\n",
    "        avg_train_citation = np.mean(epoch_loss_citation_all)\n",
    "\n",
    "        train_loss_total_list.append(avg_train_loss)\n",
    "        train_loss_count_list.append(avg_train_count)\n",
    "        train_loss_citation_list.append(avg_train_citation)\n",
    "\n",
    "        fold_train_meta_epochs.append(train_meta_info)\n",
    "        fold_train_preds_epochs.append(torch.cat(train_preds))\n",
    "        fold_train_targets_epochs.append(torch.cat(train_targets))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            val_loss_count_all = []\n",
    "            val_loss_citation_all = []\n",
    "            all_preds = []\n",
    "            all_y = []\n",
    "            val_meta_info = []  # <--- 新增：本 epoch 的 meta 資訊\n",
    "            for batch_id, batch in enumerate(val_loader):\n",
    "                x, y, idxs, years = zip(*batch)\n",
    "                x = torch.stack(x).to(device)\n",
    "                y = torch.stack(y).to(device)\n",
    "\n",
    "                preds = model(x)\n",
    "                loss_total, loss_count, loss_citation = criterion(preds, y)\n",
    "                \n",
    "                val_losses.append(loss_total.item())\n",
    "                val_loss_count_all.append(loss_count)\n",
    "                val_loss_citation_all.append(loss_citation)\n",
    "\n",
    "                all_preds.append(preds)\n",
    "                all_y.append(y)\n",
    "\n",
    "                # 💾 新增：記錄每一筆的 fold / epoch / batch / sample index\n",
    "                for i, index in enumerate(idxs):\n",
    "                    val_meta_info.append({\n",
    "                        \"fold\": fold_id + 1,\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"batch\": batch_id + 1,\n",
    "                        \"sample\": i,\n",
    "                        \"index\": index  # 原始 index 儲存以便後續追蹤\n",
    "                    })\n",
    "\n",
    "                print(f\"[Fold {fold_id+1}][Epoch {epoch+1}][Val][Batch] | Samples: {len(x)}\")\n",
    "                print(f\"  Total Loss: {loss_total.item():.4f} | Count Loss: {loss_count:.4f} | Citation Loss: {loss_citation:.4f}\")\n",
    "                print(f\"  Predictions (expm1, rounded): {np.round(torch.expm1(preds).detach().cpu().numpy())}\")\n",
    "                print(f\"  Ground truth (expm1, rounded): {np.round(torch.expm1(y).detach().cpu().numpy())}\")\n",
    "                print(f\"  Indices: {idxs}\")\n",
    "\n",
    "            fold_val_preds_epochs.append(torch.cat(all_preds).cpu())\n",
    "            fold_val_targets_epochs.append(torch.cat(all_y).cpu())\n",
    "            fold_val_meta_epochs.append(val_meta_info)\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            avg_val_count = np.mean(val_loss_count_all)\n",
    "            avg_val_citation = np.mean(val_loss_citation_all)\n",
    "\n",
    "            val_loss_total_list.append(avg_val_loss)\n",
    "            val_loss_count_list.append(avg_val_count)\n",
    "            val_loss_citation_list.append(avg_val_citation)\n",
    "\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_y = torch.cat(all_y)\n",
    "\n",
    "            # ✔ 每個 epoch 最後 summary\n",
    "            print(f\"[Fold {fold_id+1}][Epoch {epoch+1}] Summary\")\n",
    "            print(f\"  Train Avg Loss     -> Total: {avg_train_loss:.4f} | Count: {avg_train_count:.4f} | Citation: {avg_train_citation:.4f}\")\n",
    "            print(f\"  Validation Avg Loss-> Total: {avg_val_loss:.4f} | Count: {avg_val_count:.4f} | Citation: {avg_val_citation:.4f}\")\n",
    "\n",
    "        metrics = compute_metrics(all_preds.cpu(), all_y.cpu())\n",
    "        for name, vals in metrics.items():\n",
    "            print(f\"  [{name.upper()}] Metrics -> MSE: {vals['MSE']:.4f} | MAE: {vals['MAE']:.4f} | \"\n",
    "                f\"RMSE: {vals['RMSE']:.4f} | SMAPE: {vals['SMAPE']:.2f}%\")\n",
    "\n",
    "        # <-- Early Stopping 判斷 -->\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            print(f\"  ✅ Validation loss improved. Saving model.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  ❌ No improvement. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"  ⏹ Early stopping triggered at epoch {epoch+1}. Best val_loss: {best_val_loss:.4f}\")\n",
    "        if best_epoch is not None:\n",
    "            print(f\"  🔄 Best model loaded from epoch {best_epoch+1}\")\n",
    "        else:\n",
    "            print(f\"  🔄 Best model is not found (no improvement over {epochs} epochs)\")\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        # 有早停（代表中途有最佳模型）\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"  🔄 Best model loaded for final evaluation or saving.\")\n",
    "        fold_val_preds_epochs, fold_val_targets_epochs, fold_val_meta_epochs = evaluate_best_model(\n",
    "            model, val_loader, fold_id, best_epoch\n",
    "        )\n",
    "    else:\n",
    "        # 沒有早停（跑滿 epochs），此時就保留最後一輪模型，照樣 evaluate\n",
    "        print(\"  ⚠️ No best model found. Using final epoch model for evaluation.\")\n",
    "        fold_val_preds_epochs, fold_val_targets_epochs, fold_val_meta_epochs = evaluate_best_model(\n",
    "            model, val_loader, fold_id, epoch  # 此時 epoch 就是最後一輪 index\n",
    "        )\n",
    "\n",
    "    # 每個 fold 儲存進總表中\n",
    "    all_train_losses_total.append(train_loss_total_list)\n",
    "    all_train_losses_count.append(train_loss_count_list)\n",
    "    all_train_losses_citation.append(train_loss_citation_list)\n",
    "\n",
    "    all_train_preds_total.append(fold_train_preds_epochs)\n",
    "    all_train_targets_total.append(fold_train_targets_epochs)\n",
    "    all_train_meta_info_total.append(fold_train_meta_epochs)\n",
    "\n",
    "    all_val_losses_total.append(val_loss_total_list)\n",
    "    all_val_losses_count.append(val_loss_count_list)\n",
    "    all_val_losses_citation.append(val_loss_citation_list)\n",
    "\n",
    "    all_val_preds_total.append(fold_val_preds_epochs)\n",
    "    all_val_targets_total.append(fold_val_targets_epochs)\n",
    "    all_val_meta_info_total.append(fold_val_meta_epochs)\n",
    "\n",
    "    best_epoch_list.append(best_epoch + 1 if best_epoch is not None else epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198ace0",
   "metadata": {},
   "source": [
    "## compute_sample_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: shape [N, 2] numpy array, already expm1'd\n",
    "    回傳 dict，包含每筆樣本的 MSE, MAE, RMSE, SMAPE for count & citation\n",
    "    \"\"\"\n",
    "    abs_error = np.abs(y_pred - y_true)\n",
    "    squared_error = (y_pred - y_true) ** 2\n",
    "    rmse_error = np.sqrt(squared_error)\n",
    "    smape_error = 2 * abs_error / (np.abs(y_pred) + np.abs(y_true) + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"count_MSE\": squared_error[:, 0],\n",
    "        \"count_MAE\": abs_error[:, 0],\n",
    "        \"count_RMSE\": rmse_error[:, 0],\n",
    "        \"count_SMAPE\": smape_error[:, 0],\n",
    "        \"citation_MSE\": squared_error[:, 1],\n",
    "        \"citation_MAE\": abs_error[:, 1],\n",
    "        \"citation_RMSE\": rmse_error[:, 1],\n",
    "        \"citation_SMAPE\": smape_error[:, 1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117bb728",
   "metadata": {},
   "source": [
    "# Save validation result to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_summary_records, train_summary_records = [], []\n",
    "\n",
    "for fold_id in range(len(folds)):\n",
    "    # === Validation ===\n",
    "    best_epoch = best_epoch_list[fold_id] - 1\n",
    "    fold_preds = torch.expm1(all_val_preds_total[fold_id][0].cpu()).numpy()\n",
    "    fold_targets = torch.expm1(all_val_targets_total[fold_id][0].cpu()).numpy()\n",
    "    fold_meta_info = all_val_meta_info_total[fold_id][0]\n",
    "\n",
    "    metrics = compute_sample_metrics(fold_targets, fold_preds)\n",
    "\n",
    "    for i, meta in enumerate(fold_meta_info):\n",
    "        val_summary_records.append({\n",
    "            \"fold\": meta[\"fold\"],\n",
    "            \"epoch\": meta[\"epoch\"],\n",
    "            \"batch\": meta[\"batch\"],\n",
    "            \"local_sample\": meta[\"sample\"],\n",
    "            \"index\": meta[\"index\"],\n",
    "            \"file\": df.iloc[meta[\"index\"]][\"file_name\"],\n",
    "            \"val_true_count\": round(fold_targets[i, 0], 4),\n",
    "            \"val_pred_count\": round(fold_preds[i, 0], 4),\n",
    "            \"val_true_citation\": round(fold_targets[i, 1], 4),\n",
    "            \"val_pred_citation\": round(fold_preds[i, 1], 4),\n",
    "            \"val_count_MSE\": round(metrics[\"count_MSE\"][i], 4),\n",
    "            \"val_count_MAE\": round(metrics[\"count_MAE\"][i], 4),\n",
    "            \"val_count_SMAPE\": round(metrics[\"count_SMAPE\"][i], 4),\n",
    "            \"val_citation_MSE\": round(metrics[\"citation_MSE\"][i], 4),\n",
    "            \"val_citation_MAE\": round(metrics[\"citation_MAE\"][i], 4),\n",
    "            \"val_citation_SMAPE\": round(metrics[\"citation_SMAPE\"][i], 4),\n",
    "            # train 留空\n",
    "            \"train_true_count\": np.nan,\n",
    "            \"train_pred_count\": np.nan,\n",
    "            \"train_true_citation\": np.nan,\n",
    "            \"train_pred_citation\": np.nan,\n",
    "            \"train_loss_total\": np.nan,\n",
    "            \"train_loss_count\": np.nan,\n",
    "            \"train_loss_citation\": np.nan,\n",
    "            \"train_count_MSE\": np.nan,\n",
    "            \"train_count_MAE\": np.nan,\n",
    "            \"train_count_SMAPE\": np.nan,\n",
    "            \"train_citation_MSE\": np.nan,\n",
    "            \"train_citation_MAE\": np.nan,\n",
    "            \"train_citation_SMAPE\": np.nan,\n",
    "        })\n",
    "\n",
    "    # === Training ===\n",
    "    for epoch in range(len(all_train_preds_total[fold_id])):\n",
    "        fold_preds = torch.expm1(all_train_preds_total[fold_id][epoch]).numpy()\n",
    "        fold_targets = torch.expm1(all_train_targets_total[fold_id][epoch]).numpy()\n",
    "        fold_meta_info = all_train_meta_info_total[fold_id][epoch]\n",
    "\n",
    "        metrics = compute_sample_metrics(fold_targets, fold_preds)\n",
    "\n",
    "        for i, meta in enumerate(fold_meta_info):\n",
    "            train_summary_records.append({\n",
    "                \"fold\": meta[\"fold\"],\n",
    "                \"epoch\": meta[\"epoch\"],\n",
    "                \"batch\": meta[\"batch\"],\n",
    "                \"local_sample\": meta[\"sample\"],\n",
    "                \"index\": meta[\"index\"],\n",
    "                \"file\": df.iloc[meta[\"index\"]][\"file_name\"],\n",
    "                \"year\": df.iloc[meta[\"index\"]][\"year\"],\n",
    "                # val 留空\n",
    "                \"val_true_count\": np.nan,\n",
    "                \"val_pred_count\": np.nan,\n",
    "                \"val_true_citation\": np.nan,\n",
    "                \"val_pred_citation\": np.nan,\n",
    "                \"val_loss_total\": np.nan,\n",
    "                \"val_loss_count\": np.nan,\n",
    "                \"val_loss_citation\": np.nan,\n",
    "                \"val_count_MSE\": np.nan,\n",
    "                \"val_count_MAE\": np.nan,\n",
    "                \"val_count_SMAPE\": np.nan,\n",
    "                \"val_citation_MSE\": np.nan,\n",
    "                \"val_citation_MAE\": np.nan,\n",
    "                \"val_citation_SMAPE\": np.nan,\n",
    "                # train 留資料\n",
    "                \"train_true_count\": round(fold_targets[i, 0], 4),\n",
    "                \"train_pred_count\": round(fold_preds[i, 0], 4),\n",
    "                \"train_true_citation\": round(fold_targets[i, 1], 4),\n",
    "                \"train_pred_citation\": round(fold_preds[i, 1], 4),\n",
    "                \"train_loss_total\": round(all_train_losses_total[fold_id][epoch], 4),\n",
    "                \"train_loss_count\": round(all_train_losses_count[fold_id][epoch], 4),\n",
    "                \"train_count_MSE\": round(metrics[\"count_MSE\"][i].item(), 4),\n",
    "                \"train_count_MAE\": round(metrics[\"count_MAE\"][i].item(), 4),\n",
    "                \"train_count_SMAPE\": round(metrics[\"count_SMAPE\"][i].item(), 4),\n",
    "                \"train_citation_MSE\": round(metrics[\"citation_MSE\"][i].item(), 4),\n",
    "                \"train_citation_MAE\": round(metrics[\"citation_MAE\"][i].item(), 4),\n",
    "                \"train_citation_SMAPE\": round(metrics[\"citation_SMAPE\"][i].item(), 4),\n",
    "            })\n",
    "\n",
    "# 儲存 CSV\n",
    "val_summary_df = pd.DataFrame(val_summary_records)\n",
    "train_summary_df = pd.DataFrame(train_summary_records)\n",
    "combined_df = pd.concat([val_summary_df, train_summary_df], ignore_index=True)\n",
    "val_summary_df.to_csv(\"../output/mlp_mtl_v8_expanding_log_val_detailed.csv\", index=False)\n",
    "combined_df.to_csv(\"../output/mlp_mtl_v8_expanding_log_train_val_detailed.csv\", index=False)\n",
    "print(\"✅ Saved detailed validation results to 'train_val_detailed.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n====== Final Summary: Each Fold's Avg Loss (per Y) ======\")\n",
    "for fold_id in range(len(folds)):\n",
    "    avg_train_count = np.mean(all_train_losses_count[fold_id])\n",
    "    avg_train_citation = np.mean(all_train_losses_citation[fold_id])\n",
    "    avg_val_count = np.mean(all_val_losses_count[fold_id])\n",
    "    avg_val_citation = np.mean(all_val_losses_citation[fold_id])\n",
    "    \n",
    "    print(f\"[Fold {fold_id+1}]\")\n",
    "    print(f\"  Train  -> Count Loss: {avg_train_count:.4f} | Citation Loss: {avg_train_citation:.4f}\")\n",
    "    print(f\"  Valid  -> Count Loss: {avg_val_count:.4f} | Citation Loss: {avg_val_citation:.4f}\")\n",
    "\n",
    "# 全部 fold 加總平均\n",
    "mean_train_count = np.mean([np.mean(x) for x in all_train_losses_count])\n",
    "mean_train_citation = np.mean([np.mean(x) for x in all_train_losses_citation])\n",
    "mean_val_count = np.mean([np.mean(x) for x in all_val_losses_count])\n",
    "mean_val_citation = np.mean([np.mean(x) for x in all_val_losses_citation])\n",
    "\n",
    "print(\"\\n====== Final Overall Avg Loss Across 5 Folds ======\")\n",
    "print(f\"Train  -> Count Loss: {mean_train_count:.4f} | Citation Loss: {mean_train_citation:.4f}\")\n",
    "print(f\"Valid  -> Count Loss: {mean_val_count:.4f} | Citation Loss: {mean_val_citation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7787651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 合併所有 fold 的預測與 meta 資訊\n",
    "y_pred_all = torch.cat([torch.cat(fold) for fold in all_val_preds_total])\n",
    "y_true_all = torch.cat([torch.cat(fold) for fold in all_val_targets_total])\n",
    "meta_all = [meta for fold in all_val_meta_info_total for epoch in fold for meta in epoch]\n",
    "\n",
    "# 還原到原始空間\n",
    "y_pred = torch.expm1(y_pred_all).numpy()\n",
    "y_true = torch.expm1(y_true_all).numpy()\n",
    "\n",
    "# 計算每筆樣本的誤差\n",
    "abs_error = np.abs(y_pred - y_true)\n",
    "squared_error = (y_pred - y_true) ** 2\n",
    "rmse_error = np.sqrt(squared_error)\n",
    "smape = 2 * abs_error / (np.abs(y_pred) + np.abs(y_true) + 1e-8)\n",
    "\n",
    "# 印出所有資料（可改成 range(50) 減少輸出）\n",
    "print(\"\\n===== Detailed Per-Sample Metric with Meta Info =====\")\n",
    "for i in range(len(y_true)):\n",
    "    meta = meta_all[i]\n",
    "    print(f\"\\n[Sample {i}] [Fold {meta['fold']}][Epoch {meta['epoch']}][Batch {meta['batch']}][Local Sample {meta['sample']}] Index: {meta['index']}\")\n",
    "    print(f\"  COUNT    -> True: {y_true[i, 0]:.2f}, Pred: {y_pred[i, 0]:.2f}\")\n",
    "    print(f\"              MSE: {squared_error[i, 0]:.4f}, MAE: {abs_error[i, 0]:.4f}, RMSE: {rmse_error[i, 0]:.4f}, SMAPE: {smape[i, 0]*100:.2f}%\")\n",
    "    print(f\"  CITATION -> True: {y_true[i, 1]:.2f}, Pred: {y_pred[i, 1]:.2f}\")\n",
    "    print(f\"              MSE: {squared_error[i, 1]:.4f}, MAE: {abs_error[i, 1]:.4f}, RMSE: {rmse_error[i, 1]:.4f}, SMAPE: {smape[i, 1]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849121b",
   "metadata": {},
   "source": [
    "# Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curve_no_legend(train_losses, val_losses, ylabel, title, color_cycle=None):\n",
    "    \"\"\"\n",
    "    畫出主圖（訓練與驗證 loss 曲線），不含 legend。\n",
    "    回傳所有 line 的 handle，供 legend 使用。\n",
    "    \"\"\"\n",
    "    num_folds = len(train_losses)\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    handles = []\n",
    "    for fold_id in range(num_folds):\n",
    "        train_label = f\"Train - Fold {fold_id+1}\"\n",
    "        val_label = f\"Val - Fold {fold_id+1}\"\n",
    "\n",
    "        train_line, = ax.plot(train_losses[fold_id], label=train_label)\n",
    "        val_line, = ax.plot(val_losses[fold_id], linestyle='--', label=val_label)\n",
    "\n",
    "        handles.extend([train_line, val_line])\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return handles\n",
    "\n",
    "\n",
    "def plot_legend_only(handles, ncol=4, fontsize='small', figsize=(12, 2)):\n",
    "    \"\"\"\n",
    "    單獨畫 legend（圖例），供密集曲線使用者閱讀。\n",
    "    - handles: 來自 plot 的 line handles\n",
    "    - ncol: 圖例欄數\n",
    "    \"\"\"\n",
    "    fig_legend = plt.figure(figsize=figsize)\n",
    "    fig_legend.legend(handles=handles,\n",
    "                      loc='center',\n",
    "                      ncol=ncol,\n",
    "                      frameon=False,\n",
    "                      fontsize=fontsize)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_all_loss_curves_with_separate_legend(train_losses_count, val_losses_count,\n",
    "                                               train_losses_citation, val_losses_citation):\n",
    "    \"\"\"\n",
    "    一次畫出 Count & Citation 的主圖與圖例。\n",
    "    \"\"\"\n",
    "    print(\"🎨 Drawing Count Loss curve...\")\n",
    "    count_handles = plot_loss_curve_no_legend(\n",
    "        train_losses=train_losses_count,\n",
    "        val_losses=val_losses_count,\n",
    "        ylabel=\"Count Loss\",\n",
    "        title=\"Count Loss Learning Curve (Train vs. Validation)\"\n",
    "    )\n",
    "    plot_legend_only(count_handles, ncol=4)\n",
    "\n",
    "    print(\"🎨 Drawing Citation Loss curve...\")\n",
    "    citation_handles = plot_loss_curve_no_legend(\n",
    "        train_losses=train_losses_citation,\n",
    "        val_losses=val_losses_citation,\n",
    "        ylabel=\"Citation Loss\",\n",
    "        title=\"Citation Loss Learning Curve (Train vs. Validation)\"\n",
    "    )\n",
    "    plot_legend_only(citation_handles, ncol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc75e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_loss_curves_with_separate_legend(\n",
    "    all_train_losses_count, all_val_losses_count,\n",
    "    all_train_losses_citation, all_val_losses_citation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 合併所有驗證資料\n",
    "y_pred_all = torch.cat([tensor for fold in all_val_preds_total for tensor in fold])\n",
    "y_true_all = torch.cat([tensor for fold in all_val_targets_total for tensor in fold])\n",
    "\n",
    "# 還原 log1p\n",
    "y_pred_all = np.expm1(y_pred_all)\n",
    "y_true_all = np.expm1(y_true_all)\n",
    "\n",
    "# Count 圖\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_true_all[:, 0], y_pred_all[:, 0], alpha=0.5)\n",
    "plt.plot([y_true_all[:, 0].min(), y_true_all[:, 0].max()],\n",
    "         [y_pred_all[:, 0].min(), y_pred_all[:, 0].max()], 'r--')\n",
    "plt.xlabel(\"Ground Truth (Count)\")\n",
    "plt.ylabel(\"Predictions (Count)\")\n",
    "plt.title(\"Predictions vs Ground Truth (Count) – All Folds\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Citation 圖\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_true_all[:, 1], y_pred_all[:, 1], alpha=0.5)\n",
    "plt.plot([y_true_all[:, 1].min(), y_true_all[:, 1].max()],\n",
    "         [y_pred_all[:, 1].min(), y_pred_all[:, 1].max()], 'r--')\n",
    "plt.xlabel(\"Ground Truth (Citation)\")\n",
    "plt.ylabel(\"Predictions (Citation)\")\n",
    "plt.title(\"Predictions vs Ground Truth (Citation) – All Folds\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8392f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of samples: {y_true_all.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csr_env",
   "language": "python",
   "name": "csr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
