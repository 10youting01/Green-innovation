{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed16c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "df = pd.read_csv(\"/opt/hdd_1/research_hub/csr_project/Green_patent_dataset/merged_dataset/csr_embeddings_forward_count_value.csv\")\n",
    "company_ids = df['ticker'].unique()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "folds = []\n",
    "for train_idx, test_idx in kf.split(company_ids):\n",
    "    train_companies = company_ids[train_idx]\n",
    "    test_companies = company_ids[test_idx]\n",
    "    folds.append((train_companies, test_companies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24dd51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanySequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, company_list):\n",
    "        self.df = df[df['ticker'].isin(company_list)].copy()\n",
    "        self.company_groups = self.df.groupby('ticker')\n",
    "\n",
    "        self.x_seq = []\n",
    "        self.y_seq = []\n",
    "        self.lengths = []\n",
    "\n",
    "        for _, group in self.company_groups:\n",
    "            group = group.sort_values(\"year\")\n",
    "            x = torch.tensor(group[[f'dim_{i}' for i in range(1024)]].values, dtype=torch.float32)\n",
    "            y = torch.tensor(group[['patents_count', 'total_5yr_forward_citations', 'total_values_real']].values, dtype=torch.float32)\n",
    "            self.x_seq.append(x)\n",
    "            self.y_seq.append(y)\n",
    "            self.lengths.append(len(group))\n",
    "\n",
    "        # padding\n",
    "        self.x_seq = pad_sequence(self.x_seq, batch_first=True)  # [B, max_seq_len, 1024]\n",
    "        self.y_seq = pad_sequence(self.y_seq, batch_first=True)  # [B, max_seq_len, 3]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lengths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_seq[idx], self.y_seq[idx], self.lengths[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14f7fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_MTL(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.head_count = nn.Linear(hidden_dim, 1)\n",
    "        self.head_citation = nn.Linear(hidden_dim, 1)\n",
    "        self.head_value = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # [B, T, H]\n",
    "        y1 = self.head_count(lstm_out).squeeze(-1)    # [B, T]\n",
    "        y2 = self.head_citation(lstm_out).squeeze(-1)\n",
    "        y3 = self.head_value(lstm_out).squeeze(-1)\n",
    "        return y1, y2, y3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04cba247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_pred, y_true, lengths):\n",
    "    y1_pred, y2_pred, y3_pred = y_pred\n",
    "    y1_true = y_true[:,:,0]\n",
    "    y2_true = y_true[:,:,1]\n",
    "    y3_true = y_true[:,:,2]\n",
    "\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    mask = torch.arange(y_true.shape[1])[None, :].to(lengths.device) < lengths[:, None]\n",
    "\n",
    "    loss1 = loss_fn(y1_pred, y1_true) * mask\n",
    "    loss2 = loss_fn(y2_pred, y2_true) * mask\n",
    "    loss3 = loss_fn(y3_pred, y3_true) * mask\n",
    "\n",
    "    total_loss = (loss1 + loss2 + loss3).sum() / mask.sum()\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d6b0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, lengths in dataloader:\n",
    "            x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
    "            preds = model(x_batch)\n",
    "            loss = compute_loss(preds, y_batch, lengths)\n",
    "            total_loss += loss.item() * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "    return total_loss / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d5f41fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 0] Epoch 19 | Train Loss: 2712.5825 | Val Loss: 113072.1212\n",
      "[Fold 1] Epoch 19 | Train Loss: 345181.8438 | Val Loss: 194926.8376\n",
      "[Fold 2] Epoch 19 | Train Loss: 2221.2410 | Val Loss: 735801.4321\n",
      "[Fold 3] Epoch 19 | Train Loss: 4980.2764 | Val Loss: 29732.5675\n",
      "[Fold 4] Epoch 19 | Train Loss: 298545.4062 | Val Loss: 31078.2586\n"
     ]
    }
   ],
   "source": [
    "for fold_id, (train_coms, test_coms) in enumerate(folds):\n",
    "    train_dataset = CompanySequenceDataset(df, train_coms)\n",
    "    test_dataset = CompanySequenceDataset(df, test_coms)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = LSTM_MTL().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        for x_batch, y_batch, lengths in train_loader:\n",
    "            x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device) \n",
    "            preds = model(x_batch)\n",
    "            loss = compute_loss(preds, y_batch, lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "    print(f\"[Fold {fold_id}] Epoch {epoch} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a05a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
