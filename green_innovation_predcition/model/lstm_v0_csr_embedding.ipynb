{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "df = pd.read_csv(\"../data/csr_embeddings_leq2019_filled_gp.csv\")\n",
    "company_ids = df['ticker'].unique()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "folds = []\n",
    "for train_idx, test_idx in kf.split(company_ids):\n",
    "    train_companies = company_ids[train_idx]\n",
    "    test_companies = company_ids[test_idx]\n",
    "    folds.append((train_companies, test_companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24dd51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanySequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, company_list):\n",
    "        self.df = df[df['ticker'].isin(company_list)].copy()\n",
    "        self.company_groups = self.df.groupby('ticker')\n",
    "\n",
    "        self.x_seq = []\n",
    "        self.y_seq = []\n",
    "        self.lengths = []\n",
    "\n",
    "        for _, group in self.company_groups:\n",
    "            group = group.sort_values(\"year\")\n",
    "            x = torch.tensor(group[[f'dim_{i}' for i in range(1024)]].values, dtype=torch.float32)\n",
    "            y = torch.tensor(group[['patents_count', 'total_5yr_forward_citations', 'total_values_real']].values, dtype=torch.float32)\n",
    "            self.x_seq.append(x)\n",
    "            self.y_seq.append(y)\n",
    "            self.lengths.append(len(group))\n",
    "\n",
    "        # padding\n",
    "        self.x_seq = pad_sequence(self.x_seq, batch_first=True)  # [B, max_seq_len, 1024]\n",
    "        self.y_seq = pad_sequence(self.y_seq, batch_first=True)  # [B, max_seq_len, 3]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lengths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_seq[idx], self.y_seq[idx], self.lengths[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f7fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_MTL(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.head_count = nn.Linear(hidden_dim, 1)\n",
    "        self.head_citation = nn.Linear(hidden_dim, 1)\n",
    "        self.head_value = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # [B, T, H]\n",
    "        y1 = self.head_count(lstm_out).squeeze(-1)    # [B, T]\n",
    "        y2 = self.head_citation(lstm_out).squeeze(-1)\n",
    "        y3 = self.head_value(lstm_out).squeeze(-1)\n",
    "        return y1, y2, y3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04cba247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_pred, y_true, lengths):\n",
    "    y1_pred, y2_pred, y3_pred = y_pred\n",
    "    y1_true = y_true[:,:,0]\n",
    "    y2_true = y_true[:,:,1]\n",
    "    y3_true = y_true[:,:,2]\n",
    "\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    mask = torch.arange(y_true.shape[1])[None, :].to(lengths.device) < lengths[:, None]\n",
    "\n",
    "    loss1 = loss_fn(y1_pred, y1_true) * mask\n",
    "    loss2 = loss_fn(y2_pred, y2_true) * mask\n",
    "    loss3 = loss_fn(y3_pred, y3_true) * mask\n",
    "\n",
    "    total_loss = (loss1 + loss2 + loss3).sum() / mask.sum()\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd157d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_pred, y_true, mask):\n",
    "    numerator = torch.abs(y_pred - y_true)\n",
    "    denominator = (torch.abs(y_pred) + torch.abs(y_true)) / 2\n",
    "    smape = numerator / (denominator + 1e-8)\n",
    "    return (smape * mask).sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d6b0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    y_true_total = []\n",
    "    y_pred_total = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, lengths in dataloader:\n",
    "            x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device)\n",
    "            y1_pred, y2_pred, y3_pred = model(x_batch)\n",
    "            y_pred = torch.stack([y1_pred, y2_pred, y3_pred], dim=2)  # [B, T, 3]\n",
    "\n",
    "            mask = torch.arange(y_batch.shape[1])[None, :].to(lengths.device) < lengths[:, None]\n",
    "            mask = mask.unsqueeze(-1).expand_as(y_batch)  # [B, T, 3]\n",
    "\n",
    "            y_true_total.append(y_batch[mask])\n",
    "            y_pred_total.append(y_pred[mask])\n",
    "\n",
    "    # 拼接所有 batch 結果\n",
    "    y_true_all = torch.cat(y_true_total, dim=0).cpu().numpy()  # [N]\n",
    "    y_pred_all = torch.cat(y_pred_total, dim=0).cpu().numpy()  # [N]\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for i, name in enumerate([\"count\", \"citation\", \"value\"]):\n",
    "        y_true_i = y_true_all[i::3]\n",
    "        y_pred_i = y_pred_all[i::3]\n",
    "\n",
    "        mse = np.mean((y_pred_i - y_true_i) ** 2)\n",
    "        mae = np.mean(np.abs(y_pred_i - y_true_i))\n",
    "        rmse = np.sqrt(mse)\n",
    "        smape_i = np.mean(2 * np.abs(y_pred_i - y_true_i) / (np.abs(y_pred_i) + np.abs(y_true_i) + 1e-8))\n",
    "\n",
    "        metrics[name] = {\n",
    "            \"MSE\": mse,\n",
    "            \"MAE\": mae,\n",
    "            \"RMSE\": rmse,\n",
    "            \"SMAPE\": smape_i\n",
    "        }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d5f41fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 0] Epoch 99 | Train Loss: 157251.2812\n",
      "  [count] MSE: 633.98, MAE: 12.86, RMSE: 25.18, SMAPE: 1.70\n",
      "  [citation] MSE: 39366.39, MAE: 99.95, RMSE: 198.41, SMAPE: 1.78\n",
      "  [value] MSE: 201608.28, MAE: 237.24, RMSE: 449.01, SMAPE: 1.68\n",
      "[Fold 1] Epoch 99 | Train Loss: 20684.9531\n",
      "  [count] MSE: 1065.82, MAE: 12.37, RMSE: 32.65, SMAPE: 1.47\n",
      "  [citation] MSE: 50464.30, MAE: 96.13, RMSE: 224.64, SMAPE: 1.59\n",
      "  [value] MSE: 96353.79, MAE: 170.71, RMSE: 310.41, SMAPE: 1.40\n",
      "[Fold 2] Epoch 99 | Train Loss: 177633.3750\n",
      "  [count] MSE: 1187.43, MAE: 14.09, RMSE: 34.46, SMAPE: 1.55\n",
      "  [citation] MSE: 148342.44, MAE: 135.92, RMSE: 385.15, SMAPE: 1.68\n",
      "  [value] MSE: 524414.75, MAE: 257.17, RMSE: 724.16, SMAPE: 1.56\n",
      "[Fold 3] Epoch 99 | Train Loss: 18176.6875\n",
      "  [count] MSE: 160.74, MAE: 10.05, RMSE: 12.68, SMAPE: 1.58\n",
      "  [citation] MSE: 10392.84, MAE: 72.95, RMSE: 101.95, SMAPE: 1.65\n",
      "  [value] MSE: 52542.45, MAE: 161.24, RMSE: 229.22, SMAPE: 1.54\n",
      "[Fold 4] Epoch 99 | Train Loss: 14487.4736\n",
      "  [count] MSE: 1625.02, MAE: 12.35, RMSE: 40.31, SMAPE: 1.60\n",
      "  [citation] MSE: 31839.31, MAE: 76.04, RMSE: 178.44, SMAPE: 1.65\n",
      "  [value] MSE: 323635.94, MAE: 188.55, RMSE: 568.89, SMAPE: 1.66\n"
     ]
    }
   ],
   "source": [
    "for fold_id, (train_coms, test_coms) in enumerate(folds):\n",
    "    train_dataset = CompanySequenceDataset(df, train_coms)\n",
    "    test_dataset = CompanySequenceDataset(df, test_coms)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = LSTM_MTL().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for x_batch, y_batch, lengths in train_loader:\n",
    "            x_batch, y_batch, lengths = x_batch.to(device), y_batch.to(device), lengths.to(device) \n",
    "            preds = model(x_batch)\n",
    "            loss = compute_loss(preds, y_batch, lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        metrics = evaluate_metrics(model, val_loader)\n",
    "    print(f\"[Fold {fold_id}] Epoch {epoch} | Train Loss: {loss.item():.4f}\")\n",
    "    for task, task_metrics in metrics.items():\n",
    "        print(f\"  [{task}] MSE: {task_metrics['MSE']:.2f}, MAE: {task_metrics['MAE']:.2f}, RMSE: {task_metrics['RMSE']:.2f}, SMAPE: {task_metrics['SMAPE']:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a05a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csr_env",
   "language": "python",
   "name": "csr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
