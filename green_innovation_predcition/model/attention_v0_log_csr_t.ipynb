{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfdf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Sampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"../data/csr_embeddings_leq2019_citation_count_cpc_lagged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d79a3a",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed) # 控制 Python 內建的 random 模組（例如 random.shuffle()、random.randint()）。\n",
    "    np.random.seed(seed) # 控制 NumPy 所有隨機操作（例如 np.random.rand()、np.random.shuffle()）。\n",
    "    torch.manual_seed(seed) # 控制 CPU 上 PyTorch 的隨機性（例如 torch.rand()）。\n",
    "    torch.cuda.manual_seed(seed)  # for CUDA\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True  # 強制使用 deterministic（確定性）版本的 CuDNN 操作，避免某些 kernel 造成非一致性輸出。\n",
    "    torch.backends.cudnn.benchmark = False     # 關閉 CuDNN 根據輸入自動選最佳演算法的機制，避免因選到不同 kernel 而有不同結果。\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbd4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72989931",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ec7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDatasetDynamic(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, company_list, max_len=5):\n",
    "        self.samples = []\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 只保留指定公司\n",
    "        df = df[df['ticker'].isin(company_list)].copy()\n",
    "\n",
    "        for company_id, group in df.groupby('ticker'):\n",
    "            group = group.sort_values('year').reset_index(drop=True)\n",
    "\n",
    "            years = group['year'].values\n",
    "            embeddings = group[[f'dim_{i}' for i in range(1024)]].values\n",
    "            counts = group['patents_count'].values\n",
    "            citations = group['total_5yr_forward_citations'].values\n",
    "\n",
    "            # log-transform label\n",
    "            y_all = np.log1p(np.vstack([counts, citations]).T)  # shape: (N, 2)\n",
    "\n",
    "            for end in range(len(group)):\n",
    "                start = max(0, end - max_len + 1)\n",
    "                x_seq = embeddings[start:end+1]\n",
    "                y_seq = y_all[end]\n",
    "                y_year = years[end]\n",
    "\n",
    "                self.samples.append({\n",
    "                    'x_seq': torch.tensor(x_seq, dtype=torch.float32),  # 動態長度 tensor\n",
    "                    'y_seq': torch.tensor(y_seq, dtype=torch.float32),\n",
    "                    'ticker': company_id,\n",
    "                    'y_year': y_year,\n",
    "                    'index': f\"{company_id}_{years[start]}_{y_year}\"\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        return s['x_seq'], s['y_seq'], s['ticker'], s['y_year'], s['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10278f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a89f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_MTL_Stepwise(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256, head_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention Projection Layers\n",
    "        self.key_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.query_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value_layer = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Shared Sequential Layer\n",
    "        self.shared_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, head_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        # Head for patents_count\n",
    "        self.head_count = nn.Sequential(\n",
    "            nn.Linear(head_dim, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Head for citations\n",
    "        self.head_citation = nn.Sequential(\n",
    "            nn.Linear(head_dim, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        B, T_max, D = x.shape\n",
    "\n",
    "        # 預先取出 Query 位置 (最後一個有效時間步)\n",
    "        batch_indices = torch.arange(B, device=x.device)\n",
    "        last_indices = lengths - 1  # [B]\n",
    "        x_q = x[batch_indices, last_indices, :]  # [B, D]\n",
    "\n",
    "        # Q from last step\n",
    "        Q = self.query_layer(x_q).unsqueeze(1)  # [B, 1, H]\n",
    "        \n",
    "        # 針對每筆樣本，取出前 T-1 步做 K/V\n",
    "        K_all = self.key_layer(x)   # [B, T_max, H]\n",
    "        V_all = self.value_layer(x) # [B, T_max, H]\n",
    "\n",
    "        # 為了符合每筆樣本長度，mask 超過有效長度的部份 (padding masking)\n",
    "        mask = torch.arange(T_max, device=x.device).unsqueeze(0) < lengths.unsqueeze(1)  # [B, T_max]\n",
    "        mask = mask.unsqueeze(-1)  # [B, T_max, 1]\n",
    "        K = K_all * mask\n",
    "        V = V_all * mask\n",
    "\n",
    "        # Attention Scores：Q 乘以前面所有步驟的 K\n",
    "        scores = torch.matmul(Q, K.transpose(1, 2)) / np.sqrt(K.shape[-1])  # [B, 1, T_max]\n",
    "\n",
    "        # 將超過有效長度的位置設為 -inf (做 masking)\n",
    "        scores = scores.masked_fill(~mask.transpose(1, 2), float('-inf'))\n",
    "        weights = torch.softmax(scores, dim=-1)  # [B, 1, T_max]\n",
    "\n",
    "        # 加權求和得到 context\n",
    "        context = torch.matmul(weights, V).squeeze(1)  # [B, H]\n",
    "\n",
    "        # 這裡已經移除 residual，直接使用 context 作為最終表示\n",
    "        final_repr = context  # [B, H]\n",
    "\n",
    "        # Shared bottleneck\n",
    "        shared_out = self.shared_head(final_repr)  # [B, head_dim]\n",
    "\n",
    "        y1 = self.head_count(shared_out)\n",
    "        y2 = self.head_citation(shared_out)\n",
    "        return torch.cat([y1, y2], dim=1)  # [B, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d2029",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fe545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metrics (expm1 to reverse log1p) ---\n",
    "def compute_metrics(y_pred, y_true):\n",
    "    y_pred = torch.expm1(y_pred).cpu().numpy()\n",
    "    y_true = torch.expm1(y_true).cpu().numpy()\n",
    "    metrics = {}\n",
    "\n",
    "    # 加上 round\n",
    "    # y_pred = np.round(y_pred)\n",
    "    # y_true = np.round(y_true)\n",
    "    \n",
    "    for i, name in enumerate([\"count\", \"citation\"]):\n",
    "        y_p, y_t = y_pred[:, i], y_true[:, i]\n",
    "        mse = np.mean((y_p - y_t) ** 2)\n",
    "        mae = np.mean(np.abs(y_p - y_t))\n",
    "        rmse = np.sqrt(mse)\n",
    "        smape = np.mean(2 * np.abs(y_p - y_t) / (np.abs(y_p) + np.abs(y_t) + 1e-8))*100\n",
    "        metrics[name] = {\"MSE\": mse, \"MAE\": mae, \"RMSE\": rmse, \"SMAPE\": smape}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_folds_and_epochs(df, folds, epochs=3):\n",
    "    for fold_id, (train_coms, test_coms) in enumerate(folds):\n",
    "        print(f\"\\n====== Inspect Fold {fold_id+1} ======\")\n",
    "\n",
    "        # Dataset   \n",
    "        train_dataset = SlidingWindowDatasetDynamic(df, train_coms)\n",
    "        test_dataset = SlidingWindowDatasetDynamic(df, test_coms)\n",
    "\n",
    "        # DataLoader\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: x)\n",
    "        val_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\n--- Fold {fold_id+1} Epoch {epoch+1} [Train] ---\")\n",
    "            company_counter = 0\n",
    "            sample_counter = 0\n",
    "            for batch in train_loader:\n",
    "                _, _, tickers, y_years, indices = zip(*batch)\n",
    "                print(f\"Company: {tickers[0]} | Samples: {len(indices)} | Years: {min(y_years)}-{max(y_years)}\")\n",
    "                print(f\"  Indices: {indices}\")\n",
    "                company_counter += 1\n",
    "                sample_counter += len(indices)\n",
    "            print(f\"===> [Train Summary] Companies: {company_counter} | Total Samples: {sample_counter}\")\n",
    "\n",
    "            print(f\"\\n--- Fold {fold_id+1} Epoch {epoch+1} [Validation] ---\")\n",
    "            company_counter = 0\n",
    "            sample_counter = 0\n",
    "            for batch in val_loader:\n",
    "                _, _, tickers, y_years, indices = zip(*batch)\n",
    "                print(f\"Company: {tickers[0]} | Samples: {len(indices)} | Years: {min(y_years)}-{max(y_years)}\")\n",
    "                print(f\"  Indices: {indices}\")\n",
    "                company_counter += 1\n",
    "                sample_counter += len(indices)\n",
    "            print(f\"===> [Validation Summary] Companies: {company_counter} | Total Samples: {sample_counter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect_folds_and_epochs(df, folds=5, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4b969",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEPunishLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, punish_mode='', return_each=False):\n",
    "        \"\"\"\n",
    "        :param alpha: 懲罰項權重\n",
    "        :param punish_mode: 'abs', 'square', 'binary', 'huber', 'ratio', 'threshold'\n",
    "        :param return_each: 是否回傳 count loss 和 citation loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.alpha = alpha\n",
    "        self.punish_mode = punish_mode\n",
    "        self.return_each = return_each\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        pred_count, pred_citation = preds[:, 0], preds[:, 1]\n",
    "        true_count, true_citation = targets[:, 0], targets[:, 1]\n",
    "\n",
    "        loss_count = self.mse(pred_count, true_count)\n",
    "        loss_citation = self.mse(pred_citation, true_citation)\n",
    "        loss_total = loss_count + loss_citation\n",
    "\n",
    "        # Initialize punishment\n",
    "        punishment = 0.0\n",
    "\n",
    "        if self.punish_mode:\n",
    "            mask = (true_count == 0)\n",
    "            if mask.sum() > 0:\n",
    "                if self.punish_mode == 'abs':\n",
    "                    punishment = torch.sum(torch.abs(pred_citation[mask]))\n",
    "                elif self.punish_mode == 'square':\n",
    "                    punishment = torch.sum((pred_citation[mask]) ** 2)\n",
    "                elif self.punish_mode == 'binary':\n",
    "                    punishment = torch.sum((pred_citation[mask] > 1e-4).float())\n",
    "                elif self.punish_mode == 'huber':\n",
    "                    punishment = nn.functional.smooth_l1_loss(\n",
    "                        pred_citation[mask],\n",
    "                        torch.zeros_like(pred_citation[mask]),\n",
    "                        reduction='sum'\n",
    "                    )\n",
    "                elif self.punish_mode == 'ratio':\n",
    "                    safe_count = true_count + 1e-6\n",
    "                    ratio = pred_citation / safe_count\n",
    "                    punishment = torch.sum(ratio[mask])\n",
    "                elif self.punish_mode == 'threshold':\n",
    "                    threshold = 1.0\n",
    "                    over = torch.clamp(pred_citation[mask] - threshold, min=0)\n",
    "                    punishment = torch.sum(over)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported punish_mode: {self.punish_mode}\")\n",
    "\n",
    "        loss_total = loss_total + self.alpha * punishment\n",
    "\n",
    "        if self.return_each:\n",
    "            return loss_total, loss_count.item(), loss_citation.item()\n",
    "        else:\n",
    "            return loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d6638",
   "metadata": {},
   "source": [
    "## StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b49f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "company_df = df.groupby('ticker')[['patents_count', 'total_5yr_forward_citations']].mean().reset_index()\n",
    "\n",
    "# Step 1: 建立 count 是否為 0 的 flag\n",
    "company_df['has_patents'] = (company_df['patents_count'] > 0).astype(int)\n",
    "\n",
    "# Step 2: 對 citation 進行分箱（只對有產出的公司分）\n",
    "bins = [-1, 0, 10, 50, 200, np.inf]\n",
    "company_df['citation_bin'] = pd.cut(company_df['total_5yr_forward_citations'], bins=bins, labels=False)\n",
    "\n",
    "# Step 3: 建立綜合 stratify 標籤\n",
    "# 沒有產出的公司統一為 0，有產出者根據 citation_bin 分層\n",
    "company_df['stratify_label'] = company_df.apply(\n",
    "    lambda row: 0 if row['has_patents'] == 0 else row['citation_bin'] + 1,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "stratified_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "folds = []\n",
    "for train_idx, test_idx in stratified_kf.split(company_df['ticker'], company_df['stratify_label']):\n",
    "    train_coms = company_df.iloc[train_idx]['ticker'].values\n",
    "    test_coms = company_df.iloc[test_idx]['ticker'].values\n",
    "    folds.append((train_coms, test_coms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (_, test_coms) in enumerate(folds):\n",
    "    sub_df = df[df['ticker'].isin(test_coms)]\n",
    "    sample_size = len(sub_df)\n",
    "\n",
    "    # Citation 統計\n",
    "    mean_cit = sub_df['total_5yr_forward_citations'].mean()\n",
    "    std_cit = sub_df['total_5yr_forward_citations'].std()\n",
    "    max_cit = sub_df['total_5yr_forward_citations'].max()\n",
    "\n",
    "    # Count 統計\n",
    "    mean_cnt = sub_df['patents_count'].mean()\n",
    "    std_cnt = sub_df['patents_count'].std()\n",
    "    max_cnt = sub_df['patents_count'].max()\n",
    "\n",
    "    # 無專利公司比例\n",
    "    company_level = sub_df.groupby('ticker')['patents_count'].sum().reset_index()\n",
    "    no_patent_ratio = (company_level['patents_count'] == 0).mean() * 100\n",
    "\n",
    "    print(f\"[Fold {i+1}]\")\n",
    "    print(f\"  Sample Size: {sample_size}\")\n",
    "    print(f\"  Citation → mean: {mean_cit:.2f} | std: {std_cit:.2f} | max: {max_cit:.2f}\")\n",
    "    print(f\"  Count    → mean: {mean_cnt:.2f} | std: {std_cnt:.2f} | max: {max_cnt:.2f}\")\n",
    "    print(f\"  % Companies with NO patents: {no_patent_ratio:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32c288",
   "metadata": {},
   "source": [
    "## Evaluate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(model, val_loader, fold_id, best_epoch):\n",
    "    \"\"\"\n",
    "    Evaluate the best model on the validation set, returning predictions, true labels, and metadata.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    val_meta_info = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in enumerate(val_loader):\n",
    "            x_seqs, y_seqs, tickers, y_years, indices = zip(*batch)\n",
    "            lengths = torch.tensor([x.shape[0] for x in x_seqs]).to(next(model.parameters()).device)\n",
    "            x_seqs = pad_sequence(x_seqs, batch_first=True).to(next(model.parameters()).device)\n",
    "            y_seqs = torch.stack(y_seqs).to(next(model.parameters()).device)\n",
    "\n",
    "            preds = model(x_seqs, lengths)\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_y.append(y_seqs.cpu())\n",
    "\n",
    "            for i, index in enumerate(indices):\n",
    "                val_meta_info.append({\n",
    "                    \"fold\": fold_id + 1,\n",
    "                    \"epoch\": best_epoch + 1,\n",
    "                    \"batch\": batch_id + 1,\n",
    "                    \"sample\": i,\n",
    "                    \"index\": index\n",
    "                })\n",
    "\n",
    "    return [torch.cat(all_preds)], [torch.cat(all_y)], [val_meta_info]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d4f3d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "# --- Training Config ---\n",
    "## 5-fold Cross Validation\n",
    "company_ids = df['ticker'].unique()\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 初始化儲存所有 fold 的 loss\n",
    "all_train_losses_total, all_train_losses_count, all_train_losses_citation = [], [], []\n",
    "all_train_preds_total, all_train_targets_total, all_train_meta_info_total = [], [], []\n",
    "\n",
    "all_val_losses_count, all_val_losses_citation ,all_val_losses_total = [], [], []\n",
    "all_val_preds_total, all_val_targets_total, all_val_meta_info_total = [], [], []\n",
    "\n",
    "best_epoch_list = []\n",
    "\n",
    "for fold_id, (train_coms, test_coms) in enumerate(folds):\n",
    "    print(f\"\\n====== Fold {fold_id+1} ======\")\n",
    "\n",
    "    fold_train_preds_epochs, fold_train_targets_epochs, fold_train_meta_epochs = [], [], []\n",
    "    fold_val_preds_epochs, fold_val_targets_epochs, fold_val_meta_epochs = [], [], []\n",
    "    \n",
    "    # Dataset   \n",
    "    train_dataset = SlidingWindowDatasetDynamic(df, train_coms, max_len=5)\n",
    "    test_dataset = SlidingWindowDatasetDynamic(df, test_coms, max_len=5)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: x, generator=generator)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "    # Model\n",
    "    model = Attention_MTL_Stepwise().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = MSEPunishLoss(alpha=1.0, punish_mode='', return_each=True)\n",
    "\n",
    "    # 儲存每個 epoch 的 loss（每 fold 一份）\n",
    "    train_loss_total_list, train_loss_count_list, train_loss_citation_list = [], [], []\n",
    "    val_loss_total_list, val_loss_count_list, val_loss_citation_list = [], [], []\n",
    "\n",
    "    # <-- Early Stopping 初始化 -->\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50\n",
    "    best_model_state = None\n",
    "    best_epoch = None  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_losses, epoch_loss_count_all, epoch_loss_citation_all = [], [], []\n",
    "        train_preds, train_targets, train_meta_info = [], [], []\n",
    "        for batch_id, batch in enumerate(train_loader):\n",
    "            x_seqs, y_seqs, tickers, y_years, indices = zip(*batch)\n",
    "            lengths = torch.tensor([x.shape[0] for x in x_seqs]).to(device)\n",
    "            x_seqs = pad_sequence(x_seqs, batch_first=True).to(device)\n",
    "            y_seqs = torch.stack(y_seqs).to(device)\n",
    "\n",
    "            preds = model(x_seqs, lengths)\n",
    "\n",
    "            loss_total, loss_count, loss_citation = criterion(preds, y_seqs)\n",
    "            optimizer.zero_grad()\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_losses.append(loss_total.item())\n",
    "            epoch_loss_count_all.append(loss_count)\n",
    "            epoch_loss_citation_all.append(loss_citation)\n",
    "\n",
    "            train_preds.append(preds.detach().cpu())\n",
    "            train_targets.append(y_seqs.detach().cpu())\n",
    "\n",
    "            for i, index in enumerate(indices):\n",
    "                train_meta_info.append({\n",
    "                    \"fold\": fold_id + 1,\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"batch\": batch_id + 1,\n",
    "                    \"sample\": i,\n",
    "                    \"index\": index\n",
    "                })\n",
    "\n",
    "            # # ✔ 每個 batch 印 loss、predictions、truth\n",
    "            # print(f\"[Fold {fold_id+1}][Epoch {epoch+1}][Train][Batch] Company: {tickers[0]} | Samples: {len(indices)}\")\n",
    "            # print(f\"  Total Loss: {loss_total.item():.4f} | Count Loss: {loss_count:.4f} | Citation Loss: {loss_citation:.4f}\")\n",
    "            # print(f\"  Predictions (expm1, rounded): {np.round(torch.expm1(preds).detach().cpu().numpy())}\")\n",
    "            # print(f\"  Ground truth (expm1, rounded): {np.round(torch.expm1(y_seqs).detach().cpu().numpy())}\")\n",
    "            # print(f\"  Indices: {indices}\")\n",
    "\n",
    "        avg_train_loss = np.mean(epoch_losses)\n",
    "        avg_train_count = np.mean(epoch_loss_count_all)\n",
    "        avg_train_citation = np.mean(epoch_loss_citation_all)\n",
    "\n",
    "        train_loss_total_list.append(avg_train_loss)\n",
    "        train_loss_count_list.append(avg_train_count)\n",
    "        train_loss_citation_list.append(avg_train_citation)\n",
    "\n",
    "        fold_train_meta_epochs.append(train_meta_info)\n",
    "        fold_train_preds_epochs.append(torch.cat(train_preds))\n",
    "        fold_train_targets_epochs.append(torch.cat(train_targets))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            val_loss_count_all = []\n",
    "            val_loss_citation_all = []\n",
    "            all_preds = []\n",
    "            all_y = []\n",
    "            val_meta_info = []  # <--- 新增：本 epoch 的 meta 資訊\n",
    "            for batch_id, batch in enumerate(val_loader):\n",
    "                x_seqs, y_seqs, tickers, y_years, indices = zip(*batch)\n",
    "                lengths = torch.tensor([x.shape[0] for x in x_seqs]).to(device)\n",
    "                x_seqs = pad_sequence(x_seqs, batch_first=True).to(device)\n",
    "                y_seqs = torch.stack(y_seqs).to(device)\n",
    "\n",
    "                preds = model(x_seqs, lengths)\n",
    "                loss_total, loss_count, loss_citation = criterion(preds, y_seqs)\n",
    "                \n",
    "                val_losses.append(loss_total.item())\n",
    "                val_loss_count_all.append(loss_count)\n",
    "                val_loss_citation_all.append(loss_citation)\n",
    "\n",
    "                all_preds.append(preds)\n",
    "                all_y.append(y_seqs)\n",
    "\n",
    "                # 💾 新增：記錄每一筆的 fold / epoch / batch / sample index\n",
    "                for i, index in enumerate(indices):\n",
    "                    val_meta_info.append({\n",
    "                        \"fold\": fold_id + 1,\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"batch\": batch_id + 1,\n",
    "                        \"sample\": i,\n",
    "                        \"index\": index\n",
    "                    })\n",
    "\n",
    "                # ✔ 每個 batch 印 loss、predictions、truth\n",
    "                print(f\"[Fold {fold_id+1}][Epoch {epoch+1}][Val][Batch] Company: {tickers[0]} | Samples: {len(indices)}\")\n",
    "                print(f\"  Total Loss: {loss_total.item():.4f} | Count Loss: {loss_count:.4f} | Citation Loss: {loss_citation:.4f}\")\n",
    "                print(f\"  Predictions (expm1, rounded): {np.round(torch.expm1(preds).detach().cpu().numpy())}\")\n",
    "                print(f\"  Ground truth (expm1, rounded): {np.round(torch.expm1(y_seqs).detach().cpu().numpy())}\")\n",
    "                print(f\"  Indices: {indices}\")\n",
    "\n",
    "            fold_val_preds_epochs.append(torch.cat(all_preds).cpu())\n",
    "            fold_val_targets_epochs.append(torch.cat(all_y).cpu())\n",
    "            fold_val_meta_epochs.append(val_meta_info)\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            avg_val_count = np.mean(val_loss_count_all)\n",
    "            avg_val_citation = np.mean(val_loss_citation_all)\n",
    "\n",
    "            val_loss_total_list.append(avg_val_loss)\n",
    "            val_loss_count_list.append(avg_val_count)\n",
    "            val_loss_citation_list.append(avg_val_citation)\n",
    "\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_y = torch.cat(all_y)\n",
    "\n",
    "            # ✔ 每個 epoch 最後 summary\n",
    "            print(f\"[Fold {fold_id+1}][Epoch {epoch+1}] Summary\")\n",
    "            print(f\"  Train Avg Loss     -> Total: {avg_train_loss:.4f} | Count: {avg_train_count:.4f} | Citation: {avg_train_citation:.4f}\")\n",
    "            print(f\"  Validation Avg Loss-> Total: {avg_val_loss:.4f} | Count: {avg_val_count:.4f} | Citation: {avg_val_citation:.4f}\")\n",
    "\n",
    "        metrics = compute_metrics(all_preds, all_y)\n",
    "        for name, vals in metrics.items():\n",
    "            print(f\"  [{name.upper()}] Metrics -> MSE: {vals['MSE']:.4f} | MAE: {vals['MAE']:.4f} | \"\n",
    "                f\"RMSE: {vals['RMSE']:.4f} | SMAPE: {vals['SMAPE']:.2f}%\")\n",
    "\n",
    "        # <-- Early Stopping 判斷 -->\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            print(f\"  ✅ Validation loss improved. Saving model.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  ❌ No improvement. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"  ⏹ Early stopping triggered at epoch {epoch+1}. Best val_loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "        print(f\"  🔄 Best model loaded from epoch {best_epoch}\")\n",
    "\n",
    "    # 還原最佳模型狀態（可選）\n",
    "    if best_model_state is not None:\n",
    "        # 有早停（代表中途有最佳模型）\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"  🔄 Best model loaded for final evaluation or saving.\")\n",
    "        fold_val_preds_epochs, fold_val_targets_epochs, fold_val_meta_epochs = evaluate_best_model(\n",
    "            model, val_loader, fold_id, best_epoch\n",
    "        )\n",
    "    else:\n",
    "        # 沒有早停（跑滿 epochs），此時就保留最後一輪模型，照樣 evaluate\n",
    "        print(\"  ⚠️ No best model found. Using final epoch model for evaluation.\")\n",
    "        fold_val_preds_epochs, fold_val_targets_epochs, fold_val_meta_epochs = evaluate_best_model(\n",
    "            model, val_loader, fold_id, epoch  # 此時 epoch 就是最後一輪 index\n",
    "        )\n",
    "    \n",
    "    # 每個 fold 儲存進總表中\n",
    "    all_train_losses_total.append(train_loss_total_list)\n",
    "    all_train_losses_count.append(train_loss_count_list)\n",
    "    all_train_losses_citation.append(train_loss_citation_list)\n",
    "\n",
    "    all_train_preds_total.append(fold_train_preds_epochs)\n",
    "    all_train_targets_total.append(fold_train_targets_epochs)\n",
    "    all_train_meta_info_total.append(fold_train_meta_epochs)\n",
    "\n",
    "    all_val_losses_total.append(val_loss_total_list)\n",
    "    all_val_losses_count.append(val_loss_count_list)\n",
    "    all_val_losses_citation.append(val_loss_citation_list)\n",
    "\n",
    "    all_val_preds_total.append(fold_val_preds_epochs)\n",
    "    all_val_targets_total.append(fold_val_targets_epochs)\n",
    "    all_val_meta_info_total.append(fold_val_meta_epochs)\n",
    "\n",
    "    best_epoch_list.append(best_epoch + 1 if best_epoch is not None else epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d2c32",
   "metadata": {},
   "source": [
    "## compute_sample_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c300de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: shape [N, 2] numpy array, already expm1'd\n",
    "    回傳 dict，包含每筆樣本的 MSE, MAE, RMSE, SMAPE for count & citation\n",
    "    \"\"\"\n",
    "    abs_error = np.abs(y_pred - y_true)\n",
    "    squared_error = (y_pred - y_true) ** 2\n",
    "    rmse_error = np.sqrt(squared_error)\n",
    "    smape_error = 2 * abs_error / (np.abs(y_pred) + np.abs(y_true) + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"count_MSE\": squared_error[:, 0],\n",
    "        \"count_MAE\": abs_error[:, 0],\n",
    "        \"count_RMSE\": rmse_error[:, 0],\n",
    "        \"count_SMAPE\": smape_error[:, 0],\n",
    "        \"citation_MSE\": squared_error[:, 1],\n",
    "        \"citation_MAE\": abs_error[:, 1],\n",
    "        \"citation_RMSE\": rmse_error[:, 1],\n",
    "        \"citation_SMAPE\": smape_error[:, 1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30a344",
   "metadata": {},
   "source": [
    "# Save validation result to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa32c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_summary_records, train_summary_records = [], []\n",
    "\n",
    "for fold_id in range(len(folds)):\n",
    "    fold_preds = all_val_preds_total[fold_id][0]\n",
    "    fold_targets = all_val_targets_total[fold_id][0]\n",
    "    fold_meta_info = all_val_meta_info_total[fold_id][0]\n",
    "\n",
    "    # inverse log1p\n",
    "    y_pred = torch.expm1(fold_preds).numpy()\n",
    "    y_true = torch.expm1(fold_targets).numpy()\n",
    "\n",
    "    metrics = compute_sample_metrics(y_true, y_pred)\n",
    "\n",
    "    for i, meta in enumerate(fold_meta_info):\n",
    "        val_summary_records.append({\n",
    "            \"fold\": meta[\"fold\"],\n",
    "            \"epoch\": meta[\"epoch\"],\n",
    "            \"batch\": meta[\"batch\"],\n",
    "            \"local_sample\": meta[\"sample\"],\n",
    "            \"index\": meta[\"index\"],\n",
    "            \"val_true_count\": round(y_true[i, 0], 4),\n",
    "            \"val_pred_count\": round(y_pred[i, 0], 4),\n",
    "            \"val_true_citation\": round(y_true[i, 1], 4),\n",
    "            \"val_pred_citation\": round(y_pred[i, 1], 4),\n",
    "            \"val_count_MSE\": round(metrics[\"count_MSE\"][i], 4),\n",
    "            \"val_count_MAE\": round(metrics[\"count_MAE\"][i], 4),\n",
    "            \"val_count_SMAPE\": round(metrics[\"count_SMAPE\"][i], 4),\n",
    "            \"val_citation_MSE\": round(metrics[\"citation_MSE\"][i], 4),\n",
    "            \"val_citation_MAE\": round(metrics[\"citation_MAE\"][i], 4),\n",
    "            \"val_citation_SMAPE\": round(metrics[\"citation_SMAPE\"][i], 4),\n",
    "            # train 留空\n",
    "            \"train_true_count\": np.nan,\n",
    "            \"train_pred_count\": np.nan,\n",
    "            \"train_true_citation\": np.nan,\n",
    "            \"train_pred_citation\": np.nan,\n",
    "            \"train_loss_total\": np.nan,\n",
    "            \"train_loss_count\": np.nan,\n",
    "            \"train_loss_citation\": np.nan,\n",
    "            \"train_count_MSE\": np.nan,\n",
    "            \"train_count_MAE\": np.nan,\n",
    "            \"train_count_SMAPE\": np.nan,\n",
    "            \"train_citation_MSE\": np.nan,\n",
    "            \"train_citation_MAE\": np.nan,\n",
    "            \"train_citation_SMAPE\": np.nan,\n",
    "        })\n",
    "\n",
    "    # === Training ===\n",
    "    for epoch in range(len(all_train_preds_total[fold_id])):\n",
    "        fold_preds = all_train_preds_total[fold_id][epoch].cpu()\n",
    "        fold_targets = all_train_targets_total[fold_id][epoch].cpu()\n",
    "        fold_meta_info = all_train_meta_info_total[fold_id][epoch]\n",
    "\n",
    "        y_pred = torch.expm1(fold_preds).numpy()\n",
    "        y_true = torch.expm1(fold_targets).numpy()\n",
    "\n",
    "        metrics = compute_sample_metrics(y_true, y_pred)\n",
    "\n",
    "        for i, meta in enumerate(fold_meta_info):\n",
    "            train_summary_records.append({\n",
    "                \"fold\": meta[\"fold\"],\n",
    "                \"epoch\": meta[\"epoch\"],\n",
    "                \"batch\": meta[\"batch\"],\n",
    "                \"local_sample\": meta[\"sample\"],\n",
    "                \"index\": meta[\"index\"],\n",
    "                # val 留空\n",
    "                \"val_true_count\": np.nan,\n",
    "                \"val_pred_count\": np.nan,\n",
    "                \"val_true_citation\": np.nan,\n",
    "                \"val_pred_citation\": np.nan,\n",
    "                \"val_loss_total\": np.nan,\n",
    "                \"val_loss_count\": np.nan,\n",
    "                \"val_loss_citation\": np.nan,\n",
    "                \"val_count_MSE\": np.nan,\n",
    "                \"val_count_MAE\": np.nan,\n",
    "                \"val_count_SMAPE\": np.nan,\n",
    "                \"val_citation_MSE\": np.nan,\n",
    "                \"val_citation_MAE\": np.nan,\n",
    "                \"val_citation_SMAPE\": np.nan,\n",
    "                # train 留資料\n",
    "                \"train_true_count\": round(y_true[i, 0].item(), 4),\n",
    "                \"train_pred_count\": round(y_pred[i, 0].item(), 4),\n",
    "                \"train_true_citation\": round(y_true[i, 1].item(), 4),\n",
    "                \"train_pred_citation\": round(y_pred[i, 1].item(), 4),\n",
    "                \"train_count_MSE\": round(metrics[\"count_MSE\"][i].item(), 4),\n",
    "                \"train_count_MAE\": round(metrics[\"count_MAE\"][i].item(), 4),\n",
    "                \"train_count_SMAPE\": round(metrics[\"count_SMAPE\"][i].item(), 4),\n",
    "                \"train_citation_MSE\": round(metrics[\"citation_MSE\"][i].item(), 4),\n",
    "                \"train_citation_MAE\": round(metrics[\"citation_MAE\"][i].item(), 4),\n",
    "                \"train_citation_SMAPE\": round(metrics[\"citation_SMAPE\"][i].item(), 4),\n",
    "            })\n",
    "\n",
    "\n",
    "# 儲存 CSV\n",
    "val_summary_df = pd.DataFrame(val_summary_records)\n",
    "train_summary_df = pd.DataFrame(train_summary_records)\n",
    "combined_df = pd.concat([val_summary_df, train_summary_df], ignore_index=True)\n",
    "val_summary_df.to_csv(\"../output/attention_v0_log_csr_t_val_detailed.csv\", index=False)\n",
    "combined_df.to_csv(\"../output/attention_v0_log_csr_t_train_val_detailed.csv\", index=False)\n",
    "print(\"✅ Saved detailed validation results to 'train_val_detailed.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ecc56",
   "metadata": {},
   "source": [
    "# Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count Loss Learning Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "for fold_id in range(5):\n",
    "    plt.plot(all_train_losses_count[fold_id], label=f\"Train Count - Fold {fold_id+1}\")\n",
    "    plt.plot(all_val_losses_count[fold_id], label=f\"Val Count - Fold {fold_id+1}\", linestyle='--')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Count Loss\")\n",
    "plt.title(\"Count Loss Learning Curve (Train vs. Validation)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Citation Loss Learning Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "for fold_id in range(5):\n",
    "    plt.plot(all_train_losses_citation[fold_id], label=f\"Train Citation - Fold {fold_id+1}\")\n",
    "    plt.plot(all_val_losses_citation[fold_id], label=f\"Val Citation - Fold {fold_id+1}\", linestyle='--')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Citation Loss\")\n",
    "plt.title(\"Citation Loss Learning Curve (Train vs. Validation)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d44280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 合併所有驗證資料\n",
    "y_pred_all = torch.cat([tensor for fold in all_val_preds_total for tensor in fold])\n",
    "y_true_all = torch.cat([tensor for fold in all_val_targets_total for tensor in fold])\n",
    "\n",
    "# 還原 log1p\n",
    "y_pred_all = np.expm1(y_pred_all)\n",
    "y_true_all = np.expm1(y_true_all)\n",
    "\n",
    "# Count 圖\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_true_all[:, 0], y_pred_all[:, 0], alpha=0.5)\n",
    "plt.plot([y_true_all[:, 0].min(), y_true_all[:, 0].max()],\n",
    "         [y_pred_all[:, 0].min(), y_pred_all[:, 0].max()], 'r--')\n",
    "plt.xlabel(\"Ground Truth (Count)\")\n",
    "plt.ylabel(\"Predictions (Count)\")\n",
    "plt.title(\"Predictions vs Ground Truth (Count) – All Folds\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Citation 圖\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_true_all[:, 1], y_pred_all[:, 1], alpha=0.5)\n",
    "plt.plot([y_true_all[:, 1].min(), y_true_all[:, 1].max()],\n",
    "         [y_pred_all[:, 1].min(), y_pred_all[:, 1].max()], 'r--')\n",
    "plt.xlabel(\"Ground Truth (Citation)\")\n",
    "plt.ylabel(\"Predictions (Citation)\")\n",
    "plt.title(\"Predictions vs Ground Truth (Citation) – All Folds\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of samples: {y_true_all.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csr_env",
   "language": "python",
   "name": "csr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
